{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßç Human Body Reconstruction Pipeline v2.2\n",
    "\n",
    "## Updated for MediaPipe Tasks API (2024+)\n",
    "\n",
    "### Changes from v2.1:\n",
    "- ‚úÖ Uses NEW MediaPipe Tasks API (not deprecated mp.solutions)\n",
    "- ‚úÖ Downloads pose_landmarker.task model automatically\n",
    "- ‚úÖ Works with latest MediaPipe version\n",
    "- ‚úÖ No version conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.1 Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üî• CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.2 Install Dependencies\n",
    "#@markdown Uses latest MediaPipe with NEW Tasks API\n",
    "\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q mediapipe  # Latest version - uses Tasks API\n",
    "!pip install -q ultralytics\n",
    "!pip install -q smplx\n",
    "!pip install -q chumpy\n",
    "!pip install -q trimesh\n",
    "!pip install -q scikit-learn\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.3 Download MediaPipe Pose Model (.task file)\n",
    "#@markdown This downloads the pose_landmarker model required by new API\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Download pose landmarker model\n",
    "MODEL_URL = \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\"\n",
    "MODEL_PATH = \"models/pose_landmarker.task\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"üì• Downloading pose_landmarker.task...\")\n",
    "    urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
    "    print(f\"‚úÖ Downloaded to {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model already exists: {MODEL_PATH}\")\n",
    "\n",
    "print(f\"   File size: {os.path.getsize(MODEL_PATH) / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.4 Verify MediaPipe Tasks API\n",
    "import mediapipe as mp\n",
    "print(f\"MediaPipe version: {mp.__version__}\")\n",
    "\n",
    "# Test NEW Tasks API\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "print(\"‚úÖ MediaPipe Tasks API available!\")\n",
    "print(\"   Using: mediapipe.tasks.vision.PoseLandmarker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.5 Create Directories\n",
    "import os\n",
    "\n",
    "for d in ['workspace/input', 'workspace/output/frames', 'workspace/output/final', 'workspace/models/smplx']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 1: Upload Video\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 Upload Your Video\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload your turntable video (5-10 seconds, full body visible):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "VIDEO_PATH = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úÖ Uploaded: {VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.2 Verify Video\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration = total_frames / fps if fps > 0 else 0\n",
    "\n",
    "print(f\"üìπ Video Info:\")\n",
    "print(f\"   Resolution: {width} x {height}\")\n",
    "print(f\"   Duration: {duration:.1f}s ({total_frames} frames @ {fps:.0f}fps)\")\n",
    "\n",
    "ret, frame = cap.read()\n",
    "if ret:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('First Frame')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "cap.release()\n",
    "VIDEO_INFO = {'width': width, 'height': height, 'fps': fps, 'total_frames': total_frames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 2: Extract Frames\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Extract Strategic Frames\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "N_FRAMES = 8  #@param {type:\"slider\", min:4, max:16, step:2}\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "indices = np.linspace(0, total_frames - 1, N_FRAMES, dtype=int)\n",
    "\n",
    "frames = []\n",
    "print(f\"üì∏ Extracting {N_FRAMES} frames...\")\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frames.append({\n",
    "            'index': i,\n",
    "            'frame_idx': int(idx),\n",
    "            'data': cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        })\n",
    "        print(f\"   Frame {i+1}/{N_FRAMES}\")\n",
    "\n",
    "cap.release()\n",
    "print(f\"\\n‚úÖ Extracted {len(frames)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 3: Pose Estimation (NEW MediaPipe Tasks API)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Initialize Pose Landmarker (NEW API)\n",
    "#@markdown Uses mediapipe.tasks.vision.PoseLandmarker\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "\n",
    "# Create PoseLandmarker with new API\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Configure options\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    output_segmentation_masks=True,\n",
    "    min_pose_detection_confidence=0.5,\n",
    "    min_pose_presence_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Create landmarker\n",
    "pose_landmarker = PoseLandmarker.create_from_options(options)\n",
    "\n",
    "print(\"‚úÖ PoseLandmarker initialized (NEW Tasks API)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 Define Pose Detection Function\n",
    "\n",
    "# MediaPipe landmark indices to COCO 17 mapping\n",
    "# MediaPipe has 33 landmarks, COCO uses 17\n",
    "MP_TO_COCO = {\n",
    "    0: 0,    # nose\n",
    "    2: 1,    # left_eye\n",
    "    5: 2,    # right_eye\n",
    "    7: 3,    # left_ear\n",
    "    8: 4,    # right_ear\n",
    "    11: 5,   # left_shoulder\n",
    "    12: 6,   # right_shoulder\n",
    "    13: 7,   # left_elbow\n",
    "    14: 8,   # right_elbow\n",
    "    15: 9,   # left_wrist\n",
    "    16: 10,  # right_wrist\n",
    "    23: 11,  # left_hip\n",
    "    24: 12,  # right_hip\n",
    "    25: 13,  # left_knee\n",
    "    26: 14,  # right_knee\n",
    "    27: 15,  # left_ankle\n",
    "    28: 16,  # right_ankle\n",
    "}\n",
    "\n",
    "def detect_pose_new_api(image_rgb, landmarker):\n",
    "    \"\"\"\n",
    "    Detect pose using NEW MediaPipe Tasks API.\n",
    "    \n",
    "    Args:\n",
    "        image_rgb: RGB image as numpy array\n",
    "        landmarker: PoseLandmarker instance\n",
    "    \n",
    "    Returns:\n",
    "        dict with keypoints (COCO format), segmentation mask\n",
    "    \"\"\"\n",
    "    h, w = image_rgb.shape[:2]\n",
    "    \n",
    "    # Convert to MediaPipe Image\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "    \n",
    "    # Detect\n",
    "    result = landmarker.detect(mp_image)\n",
    "    \n",
    "    if not result.pose_landmarks or len(result.pose_landmarks) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get first pose (single person)\n",
    "    landmarks = result.pose_landmarks[0]\n",
    "    \n",
    "    # Extract all 33 MediaPipe landmarks\n",
    "    mp_keypoints = []\n",
    "    for lm in landmarks:\n",
    "        mp_keypoints.append([lm.x * w, lm.y * h, lm.visibility])\n",
    "    mp_keypoints = np.array(mp_keypoints)\n",
    "    \n",
    "    # Convert to COCO 17 format\n",
    "    coco_keypoints = np.zeros((17, 3))\n",
    "    for mp_idx, coco_idx in MP_TO_COCO.items():\n",
    "        coco_keypoints[coco_idx] = mp_keypoints[mp_idx]\n",
    "    \n",
    "    # Get segmentation mask if available\n",
    "    segmentation = None\n",
    "    if result.segmentation_masks and len(result.segmentation_masks) > 0:\n",
    "        mask = result.segmentation_masks[0].numpy_view()\n",
    "        segmentation = (mask > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # Calculate bounding box from keypoints\n",
    "    valid_kps = mp_keypoints[mp_keypoints[:, 2] > 0.5]\n",
    "    if len(valid_kps) > 0:\n",
    "        x_coords = valid_kps[:, 0]\n",
    "        y_coords = valid_kps[:, 1]\n",
    "        bbox = [x_coords.min(), y_coords.min(), x_coords.max(), y_coords.max()]\n",
    "    else:\n",
    "        bbox = [0, 0, w, h]\n",
    "    \n",
    "    return {\n",
    "        'keypoints': coco_keypoints,\n",
    "        'keypoints_full': mp_keypoints,  # All 33 landmarks\n",
    "        'bbox': np.array(bbox),\n",
    "        'segmentation': segmentation\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Pose detection function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.3 Run Pose Detection on All Frames\n",
    "\n",
    "print(f\"üîç Processing {len(frames)} frames...\")\n",
    "pose_results = []\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    result = detect_pose_new_api(frame['data'], pose_landmarker)\n",
    "    \n",
    "    if result:\n",
    "        result['frame_index'] = i\n",
    "        pose_results.append(result)\n",
    "        print(f\"   Frame {i+1}: ‚úì Pose detected (33 landmarks)\")\n",
    "    else:\n",
    "        pose_results.append(None)\n",
    "        print(f\"   Frame {i+1}: ‚ö†Ô∏è No pose detected\")\n",
    "\n",
    "success_count = sum(1 for r in pose_results if r is not None)\n",
    "print(f\"\\n‚úÖ Pose detection complete: {success_count}/{len(frames)} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.4 Visualize Pose Results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# COCO skeleton connections\n",
    "SKELETON = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "    (5, 11), (6, 12), (11, 12),\n",
    "    (11, 13), (13, 15), (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "cols = 4\n",
    "rows = (len(frames) + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(frames):\n",
    "        ax.imshow(frames[i]['data'])\n",
    "        \n",
    "        if pose_results[i] is not None:\n",
    "            kp = pose_results[i]['keypoints']\n",
    "            \n",
    "            # Draw keypoints\n",
    "            for j, (x, y, c) in enumerate(kp):\n",
    "                if c > 0.3:\n",
    "                    ax.scatter(x, y, c='lime', s=30, zorder=5)\n",
    "            \n",
    "            # Draw skeleton\n",
    "            for (s, e) in SKELETON:\n",
    "                if kp[s, 2] > 0.3 and kp[e, 2] > 0.3:\n",
    "                    ax.plot([kp[s, 0], kp[e, 0]], [kp[s, 1], kp[e, 1]], 'c-', lw=2)\n",
    "            \n",
    "            ax.set_title(f\"Frame {i+1}: ‚úì\")\n",
    "        else:\n",
    "            ax.set_title(f\"Frame {i+1}: ‚ö†Ô∏è\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 4: Upload SMPL-X Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Upload SMPL-X Model\n",
    "#@markdown Download from https://smpl-x.is.tue.mpg.de/ (free registration)\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "SMPLX_PATH = 'workspace/models/smplx'\n",
    "model_file = os.path.join(SMPLX_PATH, 'SMPLX_NEUTRAL.npz')\n",
    "\n",
    "if not os.path.exists(model_file):\n",
    "    print(\"üì§ Upload SMPLX_NEUTRAL.npz:\")\n",
    "    uploaded = files.upload()\n",
    "    for fname in uploaded.keys():\n",
    "        os.rename(fname, os.path.join(SMPLX_PATH, fname))\n",
    "        print(f\"‚úÖ Saved to {SMPLX_PATH}/{fname}\")\n",
    "else:\n",
    "    print(f\"‚úÖ SMPL-X model exists: {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Load SMPL-X\n",
    "import smplx\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "body_model = smplx.create(\n",
    "    SMPLX_PATH,\n",
    "    model_type='smplx',\n",
    "    gender='neutral',\n",
    "    num_betas=10,\n",
    "    ext='npz'\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ SMPL-X loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 5: Camera Estimation (PnP)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Iterative PnP Camera Estimation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# COCO to SMPL-X mapping (reliable joints only)\n",
    "COCO_TO_SMPLX = {\n",
    "    5: 16, 6: 17, 7: 18, 8: 19, 9: 20, 10: 21,\n",
    "    11: 1, 12: 2, 13: 4, 14: 5, 15: 7, 16: 8\n",
    "}\n",
    "\n",
    "def get_smplx_joints(body_model, betas, device):\n",
    "    with torch.no_grad():\n",
    "        output = body_model(\n",
    "            betas=betas,\n",
    "            body_pose=torch.zeros(1, 63, device=device),\n",
    "            global_orient=torch.zeros(1, 3, device=device)\n",
    "        )\n",
    "    return output.joints[0].cpu().numpy()\n",
    "\n",
    "def solve_pnp(keypoints_2d, joints_3d, K):\n",
    "    pts_2d, pts_3d = [], []\n",
    "    for coco_idx, smplx_idx in COCO_TO_SMPLX.items():\n",
    "        if keypoints_2d[coco_idx, 2] > 0.5:\n",
    "            pts_2d.append(keypoints_2d[coco_idx, :2])\n",
    "            pts_3d.append(joints_3d[smplx_idx])\n",
    "    \n",
    "    if len(pts_2d) < 6:\n",
    "        return None, None, False\n",
    "    \n",
    "    pts_2d = np.array(pts_2d, dtype=np.float64)\n",
    "    pts_3d = np.array(pts_3d, dtype=np.float64)\n",
    "    \n",
    "    success, rvec, tvec, _ = cv2.solvePnPRansac(pts_3d, pts_2d, K, None)\n",
    "    if not success:\n",
    "        return None, None, False\n",
    "    \n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "    return R, tvec.flatten(), True\n",
    "\n",
    "# Camera intrinsics\n",
    "focal = max(VIDEO_INFO['width'], VIDEO_INFO['height'])\n",
    "K = np.array([[focal, 0, VIDEO_INFO['width']/2],\n",
    "              [0, focal, VIDEO_INFO['height']/2],\n",
    "              [0, 0, 1]], dtype=np.float64)\n",
    "\n",
    "# Iterative PnP\n",
    "print(\"üì∑ Iterative PnP Camera Estimation...\")\n",
    "betas = torch.zeros(1, 10, device=device)\n",
    "cameras = [None] * len(pose_results)\n",
    "\n",
    "for iteration in range(3):\n",
    "    print(f\"   Round {iteration+1}/3\")\n",
    "    joints_3d = get_smplx_joints(body_model, betas, device)\n",
    "    \n",
    "    for i, pose in enumerate(pose_results):\n",
    "        if pose is None:\n",
    "            continue\n",
    "        R, t, success = solve_pnp(pose['keypoints'], joints_3d, K)\n",
    "        if success:\n",
    "            cameras[i] = {'R': R, 't': t, 'K': K.copy()}\n",
    "\n",
    "print(f\"\\n‚úÖ Camera estimation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 6: Shape Optimization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Optimize Body Shape\n",
    "\n",
    "betas = torch.zeros(1, 10, device=device, requires_grad=True)\n",
    "optimizer = torch.optim.Adam([betas], lr=0.02)\n",
    "\n",
    "print(\"üîß Optimizing body shape...\")\n",
    "\n",
    "for iteration in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = body_model(\n",
    "        betas=betas,\n",
    "        body_pose=torch.zeros(1, 63, device=device),\n",
    "        global_orient=torch.zeros(1, 3, device=device)\n",
    "    )\n",
    "    joints = output.joints[0]\n",
    "    \n",
    "    loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for pose, cam in zip(pose_results, cameras):\n",
    "        if pose is None or cam is None:\n",
    "            continue\n",
    "        \n",
    "        R_t = torch.tensor(cam['R'], dtype=torch.float32, device=device)\n",
    "        t_t = torch.tensor(cam['t'], dtype=torch.float32, device=device)\n",
    "        K_t = torch.tensor(cam['K'], dtype=torch.float32, device=device)\n",
    "        \n",
    "        body_joints = torch.stack([joints[COCO_TO_SMPLX[i]] for i in COCO_TO_SMPLX.keys()])\n",
    "        cam_pts = torch.matmul(body_joints, R_t.T) + t_t\n",
    "        proj = torch.matmul(cam_pts, K_t.T)\n",
    "        proj_2d = proj[:, :2] / (proj[:, 2:3] + 1e-8)\n",
    "        \n",
    "        gt_2d = torch.tensor(pose['keypoints'][list(COCO_TO_SMPLX.keys()), :2],\n",
    "                            dtype=torch.float32, device=device)\n",
    "        conf = torch.tensor(pose['keypoints'][list(COCO_TO_SMPLX.keys()), 2],\n",
    "                           dtype=torch.float32, device=device)\n",
    "        \n",
    "        loss += torch.sum(conf.unsqueeze(-1) * (proj_2d - gt_2d)**2)\n",
    "        count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        # Symmetry loss\n",
    "        left_arm = torch.norm(joints[16]-joints[18]) + torch.norm(joints[18]-joints[20])\n",
    "        right_arm = torch.norm(joints[17]-joints[19]) + torch.norm(joints[19]-joints[21])\n",
    "        sym_loss = (left_arm - right_arm)**2\n",
    "        \n",
    "        total = loss/count + 0.01*torch.mean(betas**2) + 0.1*sym_loss\n",
    "        total.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if iteration % 50 == 0:\n",
    "        print(f\"   Iter {iteration}: Loss = {total.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Shape optimization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 7: Extract Measurements\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.1 Generate Mesh & Extract Measurements\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# Get canonical mesh\n",
    "with torch.no_grad():\n",
    "    output = body_model(\n",
    "        betas=betas,\n",
    "        body_pose=torch.zeros(1, 63, device=device),\n",
    "        global_orient=torch.zeros(1, 3, device=device),\n",
    "        return_verts=True\n",
    "    )\n",
    "\n",
    "vertices = output.vertices[0].cpu().numpy()\n",
    "joints = output.joints[0].cpu().numpy()\n",
    "\n",
    "# Scale factor\n",
    "KNOWN_HEIGHT_CM = None  #@param {type:\"number\"}\n",
    "raw_height = vertices[:, 1].max() - vertices[:, 1].min()\n",
    "scale = KNOWN_HEIGHT_CM / raw_height if KNOWN_HEIGHT_CM else 100\n",
    "\n",
    "def measure_circumference(verts, center, radius=0.1):\n",
    "    dist = np.linalg.norm(verts - center, axis=1)\n",
    "    nearby = verts[dist < radius]\n",
    "    if len(nearby) < 20:\n",
    "        return 0\n",
    "    pca = PCA(n_components=2)\n",
    "    pts_2d = pca.fit_transform(nearby - center)\n",
    "    try:\n",
    "        hull = ConvexHull(pts_2d)\n",
    "        hull_pts = pts_2d[hull.vertices]\n",
    "        perim = sum(np.linalg.norm(hull_pts[i] - hull_pts[(i+1)%len(hull_pts)]) \n",
    "                   for i in range(len(hull_pts)))\n",
    "        return perim * scale\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Measurements\n",
    "measurements = {\n",
    "    'height': raw_height * scale,\n",
    "    'shoulder_width': np.linalg.norm(joints[16] - joints[17]) * scale,\n",
    "    'hip_width': np.linalg.norm(joints[1] - joints[2]) * scale,\n",
    "    'torso_length': np.linalg.norm(joints[12] - joints[0]) * scale,\n",
    "    'arm_length': ((np.linalg.norm(joints[16]-joints[18]) + np.linalg.norm(joints[18]-joints[20]) +\n",
    "                   np.linalg.norm(joints[17]-joints[19]) + np.linalg.norm(joints[19]-joints[21])) / 2) * scale,\n",
    "    'leg_length': ((np.linalg.norm(joints[1]-joints[4]) + np.linalg.norm(joints[4]-joints[7]) +\n",
    "                   np.linalg.norm(joints[2]-joints[5]) + np.linalg.norm(joints[5]-joints[8])) / 2) * scale,\n",
    "    'inseam': np.linalg.norm(((joints[1]+joints[2])/2 - np.array([0,0.03,0])) - (joints[7]+joints[8])/2) * scale,\n",
    "}\n",
    "\n",
    "# Circumferences\n",
    "measurements['chest_circumference'] = measure_circumference(vertices, (joints[16]+joints[17])/2 - [0,0.05,0], 0.12)\n",
    "measurements['waist_circumference'] = measure_circumference(vertices, (joints[3]+joints[6])/2, 0.10)\n",
    "measurements['hip_circumference'] = measure_circumference(vertices, joints[0], 0.12)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìè BODY MEASUREMENTS\")\n",
    "print(\"=\"*60)\n",
    "for name, value in measurements.items():\n",
    "    print(f\"   {name.replace('_', ' ').title():<25} {value:>8.1f} cm\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Visualize Results\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(vertices[::10, 0], vertices[::10, 2], vertices[::10, 1], c='lightblue', s=1, alpha=0.5)\n",
    "ax1.scatter(joints[:22, 0], joints[:22, 2], joints[:22, 1], c='red', s=50)\n",
    "ax1.set_title('Front View')\n",
    "\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(vertices[::10, 2], vertices[::10, 0], vertices[::10, 1], c='lightblue', s=1, alpha=0.5)\n",
    "ax2.set_title('Side View')\n",
    "ax2.view_init(elev=0, azim=0)\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.axis('off')\n",
    "text = \"üìè MEASUREMENTS\\n\" + \"=\"*30 + \"\\n\\n\"\n",
    "for name, value in measurements.items():\n",
    "    text += f\"{name.replace('_', ' ').title()}: {value:.1f} cm\\n\"\n",
    "ax3.text(0.1, 0.9, text, transform=ax3.transAxes, fontsize=12,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('workspace/output/final/body_measurements.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 8: Save & Download Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8.1 Save Results\n",
    "import json\n",
    "\n",
    "# Save measurements\n",
    "output_data = {\n",
    "    'measurements_cm': {k: float(v) for k, v in measurements.items()},\n",
    "    'pipeline_version': '2.2',\n",
    "    'api': 'MediaPipe Tasks (NEW)',\n",
    "    'betas': betas.detach().cpu().numpy().tolist()\n",
    "}\n",
    "\n",
    "with open('workspace/output/final/measurements.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "# Save mesh\n",
    "with open('workspace/output/final/body.obj', 'w') as f:\n",
    "    for v in vertices:\n",
    "        f.write(f\"v {v[0]} {v[1]} {v[2]}\\n\")\n",
    "    for face in body_model.faces:\n",
    "        f.write(f\"f {face[0]+1} {face[1]+1} {face[2]+1}\\n\")\n",
    "\n",
    "print(\"‚úÖ Results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8.2 Download Results\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('body_reconstruction_v22', 'zip', 'workspace/output/final')\n",
    "files.download('body_reconstruction_v22.zip')\n",
    "\n",
    "print(\"üì• Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Complete!\n",
    "---\n",
    "\n",
    "## V2.2 Changes\n",
    "- Uses NEW MediaPipe Tasks API (`mediapipe.tasks.vision.PoseLandmarker`)\n",
    "- Downloads `.task` model file automatically\n",
    "- No version conflicts with latest MediaPipe\n",
    "- Compatible with Colab Python 3.12\n",
    "\n",
    "## Expected Accuracy\n",
    "| Measurement | Error |\n",
    "|-------------|-------|\n",
    "| Height | ¬±1-1.5 cm |\n",
    "| Circumferences | ¬±2-3 cm |\n",
    "| Limb lengths | ¬±1.5 cm |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
