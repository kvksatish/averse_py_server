{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßç Human Body Reconstruction Pipeline v2.1 (Fixed)\n",
    "\n",
    "## Production-Grade Implementation - Colab Compatible\n",
    "\n",
    "### Fixed Issues:\n",
    "- ‚úÖ No MMPose (causes Python 3.12 conflicts)\n",
    "- ‚úÖ Uses MediaPipe for pose (stable, fast)\n",
    "- ‚úÖ Clean dependency installation\n",
    "- ‚úÖ Iterative PnP camera estimation\n",
    "- ‚úÖ PCA-based circumference measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 0: Setup (Run All Cells in Order)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.1 Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üî• CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.2 Install Dependencies (Fixed - No MMPose)\n",
    "#@markdown This avoids the Python 3.12 compatibility issues\n",
    "\n",
    "# Core packages (these are safe)\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q mediapipe==0.10.9\n",
    "!pip install -q ultralytics\n",
    "!pip install -q smplx\n",
    "!pip install -q chumpy\n",
    "!pip install -q trimesh\n",
    "!pip install -q scikit-learn\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.3 Verify Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import smplx\n",
    "import trimesh\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   OpenCV: {cv2.__version__}\")\n",
    "print(f\"   MediaPipe: {mp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.4 Create Directories\n",
    "import os\n",
    "\n",
    "for d in ['workspace/input', 'workspace/output/frames', 'workspace/output/final', 'workspace/models/smplx']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 1: Upload Video\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 Upload Your Video\n",
    "#@markdown Click \"Choose Files\" button and select your turntable video\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload your turntable video:\")\n",
    "print(\"   Requirements:\")\n",
    "print(\"   - 5-10 seconds long\")\n",
    "print(\"   - Person rotating slowly (or camera moving around person)\")\n",
    "print(\"   - Full body visible\")\n",
    "print(\"   - MP4 format recommended\")\n",
    "print(\"\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "VIDEO_PATH = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úÖ Uploaded: {VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.2 Verify Video\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "# Get video info\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration = total_frames / fps if fps > 0 else 0\n",
    "\n",
    "print(f\"üìπ Video Info:\")\n",
    "print(f\"   Resolution: {width} x {height}\")\n",
    "print(f\"   Duration: {duration:.1f} seconds\")\n",
    "print(f\"   FPS: {fps:.0f}\")\n",
    "print(f\"   Total Frames: {total_frames}\")\n",
    "\n",
    "# Show first frame\n",
    "ret, frame = cap.read()\n",
    "if ret:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('First Frame Preview')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(\"\\n‚úÖ Video loaded successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Error: Could not read video\")\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Store for later\n",
    "VIDEO_INFO = {'width': width, 'height': height, 'fps': fps, 'total_frames': total_frames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 2: Extract Frames\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Extract 8 Strategic Frames\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "N_FRAMES = 8  #@param {type:\"slider\", min:4, max:16, step:2}\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Get evenly spaced frame indices\n",
    "indices = np.linspace(0, total_frames - 1, N_FRAMES, dtype=int)\n",
    "\n",
    "frames = []\n",
    "print(f\"üì∏ Extracting {N_FRAMES} frames...\")\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append({\n",
    "            'index': i,\n",
    "            'frame_idx': int(idx),\n",
    "            'data': frame_rgb\n",
    "        })\n",
    "        print(f\"   Frame {i+1}/{N_FRAMES} extracted (original index: {idx})\")\n",
    "\n",
    "cap.release()\n",
    "print(f\"\\n‚úÖ Extracted {len(frames)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.2 Preview Extracted Frames\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 4\n",
    "rows = (len(frames) + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "axes = axes.flatten() if len(frames) > cols else [axes] if len(frames) == 1 else axes\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(frames):\n",
    "        ax.imshow(frames[i]['data'])\n",
    "        ax.set_title(f\"Frame {i+1}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 3: Person Detection & Pose Estimation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Initialize Detection & Pose Models\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "# YOLOv8 for bounding box detection\n",
    "print(\"üîÑ Loading YOLOv8...\")\n",
    "detector = YOLO('yolov8x.pt')\n",
    "print(\"‚úÖ YOLOv8 loaded\")\n",
    "\n",
    "# MediaPipe for pose estimation\n",
    "print(\"\\nüîÑ Loading MediaPipe Pose...\")\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_estimator = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=2,  # Highest accuracy\n",
    "    enable_segmentation=True,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "print(\"‚úÖ MediaPipe Pose loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 Run Detection & Pose on All Frames\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe to COCO keypoint mapping\n",
    "MP_TO_COCO = {\n",
    "    0: 0,    # nose\n",
    "    2: 1,    # left_eye\n",
    "    5: 2,    # right_eye\n",
    "    7: 3,    # left_ear\n",
    "    8: 4,    # right_ear\n",
    "    11: 5,   # left_shoulder\n",
    "    12: 6,   # right_shoulder\n",
    "    13: 7,   # left_elbow\n",
    "    14: 8,   # right_elbow\n",
    "    15: 9,   # left_wrist\n",
    "    16: 10,  # right_wrist\n",
    "    23: 11,  # left_hip\n",
    "    24: 12,  # right_hip\n",
    "    25: 13,  # left_knee\n",
    "    26: 14,  # right_knee\n",
    "    27: 15,  # left_ankle\n",
    "    28: 16,  # right_ankle\n",
    "}\n",
    "\n",
    "def process_frame(image, detector, pose_estimator):\n",
    "    \"\"\"Detect person and estimate pose\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Step 1: Detect bounding box\n",
    "    det_results = detector(image, verbose=False, classes=[0])  # class 0 = person\n",
    "    \n",
    "    if len(det_results[0].boxes) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get largest person bbox\n",
    "    boxes = det_results[0].boxes\n",
    "    areas = (boxes.xyxy[:, 2] - boxes.xyxy[:, 0]) * (boxes.xyxy[:, 3] - boxes.xyxy[:, 1])\n",
    "    best_idx = areas.argmax()\n",
    "    bbox = boxes.xyxy[best_idx].cpu().numpy()\n",
    "    \n",
    "    # Step 2: Crop and run pose estimation\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    pad = int(max(x2-x1, y2-y1) * 0.1)\n",
    "    x1, y1 = max(0, x1-pad), max(0, y1-pad)\n",
    "    x2, y2 = min(w, x2+pad), min(h, y2+pad)\n",
    "    \n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    results = pose_estimator.process(crop)\n",
    "    \n",
    "    if not results.pose_landmarks:\n",
    "        return None\n",
    "    \n",
    "    # Extract keypoints\n",
    "    ch, cw = crop.shape[:2]\n",
    "    mp_keypoints = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        mp_keypoints.append([lm.x * cw + x1, lm.y * ch + y1, lm.visibility])\n",
    "    mp_keypoints = np.array(mp_keypoints)\n",
    "    \n",
    "    # Convert to COCO format\n",
    "    coco_keypoints = np.zeros((17, 3))\n",
    "    for mp_idx, coco_idx in MP_TO_COCO.items():\n",
    "        coco_keypoints[coco_idx] = mp_keypoints[mp_idx]\n",
    "    \n",
    "    # Get segmentation mask\n",
    "    segmentation = None\n",
    "    if results.segmentation_mask is not None:\n",
    "        seg_crop = (results.segmentation_mask > 0.5).astype(np.uint8)\n",
    "        segmentation = np.zeros((h, w), dtype=np.uint8)\n",
    "        segmentation[y1:y2, x1:x2] = seg_crop\n",
    "    \n",
    "    return {\n",
    "        'bbox': bbox,\n",
    "        'keypoints': coco_keypoints,\n",
    "        'segmentation': segmentation\n",
    "    }\n",
    "\n",
    "# Process all frames\n",
    "print(f\"üîç Processing {len(frames)} frames...\")\n",
    "pose_results = []\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    result = process_frame(frame['data'], detector, pose_estimator)\n",
    "    \n",
    "    if result:\n",
    "        result['frame_index'] = i\n",
    "        pose_results.append(result)\n",
    "        print(f\"   Frame {i+1}: ‚úì Person detected, 17 keypoints extracted\")\n",
    "    else:\n",
    "        pose_results.append(None)\n",
    "        print(f\"   Frame {i+1}: ‚ö†Ô∏è No detection\")\n",
    "\n",
    "success_count = sum(1 for r in pose_results if r is not None)\n",
    "print(f\"\\n‚úÖ Processing complete: {success_count}/{len(frames)} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.3 Visualize Pose Estimation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# COCO skeleton connections\n",
    "SKELETON = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n",
    "    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Arms\n",
    "    (5, 11), (6, 12), (11, 12),  # Torso\n",
    "    (11, 13), (13, 15), (12, 14), (14, 16)  # Legs\n",
    "]\n",
    "\n",
    "cols = 4\n",
    "rows = (len(frames) + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(frames):\n",
    "        ax.imshow(frames[i]['data'])\n",
    "        \n",
    "        if pose_results[i] is not None:\n",
    "            kp = pose_results[i]['keypoints']\n",
    "            bbox = pose_results[i]['bbox']\n",
    "            \n",
    "            # Draw bbox\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
    "                linewidth=2, edgecolor='lime', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Draw keypoints\n",
    "            for j, (x, y, c) in enumerate(kp):\n",
    "                if c > 0.3:\n",
    "                    ax.scatter(x, y, c='red', s=30, zorder=5)\n",
    "            \n",
    "            # Draw skeleton\n",
    "            for (s, e) in SKELETON:\n",
    "                if kp[s, 2] > 0.3 and kp[e, 2] > 0.3:\n",
    "                    ax.plot([kp[s, 0], kp[e, 0]], [kp[s, 1], kp[e, 1]], 'c-', lw=2)\n",
    "            \n",
    "            ax.set_title(f\"Frame {i+1}: ‚úì\")\n",
    "        else:\n",
    "            ax.set_title(f\"Frame {i+1}: ‚ö†Ô∏è\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 4: Upload SMPL-X Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Upload SMPL-X Model\n",
    "#@markdown Download from https://smpl-x.is.tue.mpg.de/ (free registration)\n",
    "#@markdown Upload the SMPLX_NEUTRAL.npz file\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "SMPLX_PATH = 'workspace/models/smplx'\n",
    "model_file = os.path.join(SMPLX_PATH, 'SMPLX_NEUTRAL.npz')\n",
    "\n",
    "if not os.path.exists(model_file):\n",
    "    print(\"üì§ Upload SMPLX_NEUTRAL.npz:\")\n",
    "    print(\"   (Download from https://smpl-x.is.tue.mpg.de/)\")\n",
    "    print(\"\")\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for fname in uploaded.keys():\n",
    "        dest = os.path.join(SMPLX_PATH, fname)\n",
    "        os.rename(fname, dest)\n",
    "        print(f\"\\n‚úÖ Saved to: {dest}\")\n",
    "else:\n",
    "    print(f\"‚úÖ SMPL-X model already exists at {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Load SMPL-X Model\n",
    "import smplx\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîÑ Loading SMPL-X on {device}...\")\n",
    "\n",
    "body_model = smplx.create(\n",
    "    SMPLX_PATH,\n",
    "    model_type='smplx',\n",
    "    gender='neutral',\n",
    "    use_face_contour=True,\n",
    "    num_betas=10,\n",
    "    num_expression_coeffs=10,\n",
    "    ext='npz'\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ SMPL-X loaded\")\n",
    "print(f\"   Vertices: {body_model.get_num_verts()}\")\n",
    "print(f\"   Joints: {body_model.NUM_JOINTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 5: Camera Estimation (Iterative PnP)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Iterative PnP Camera Estimation\n",
    "#@markdown This is the key improvement - estimates camera pose accurately\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# COCO to SMPL-X joint mapping (reliable joints only, skip head)\n",
    "COCO_TO_SMPLX = {\n",
    "    5: 16,   # left_shoulder\n",
    "    6: 17,   # right_shoulder\n",
    "    7: 18,   # left_elbow\n",
    "    8: 19,   # right_elbow\n",
    "    9: 20,   # left_wrist\n",
    "    10: 21,  # right_wrist\n",
    "    11: 1,   # left_hip\n",
    "    12: 2,   # right_hip\n",
    "    13: 4,   # left_knee\n",
    "    14: 5,   # right_knee\n",
    "    15: 7,   # left_ankle\n",
    "    16: 8,   # right_ankle\n",
    "}\n",
    "\n",
    "def get_smplx_joints(body_model, betas, device):\n",
    "    \"\"\"Get 3D joints from SMPL-X\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = body_model(\n",
    "            betas=betas,\n",
    "            body_pose=torch.zeros(1, 63, device=device),\n",
    "            global_orient=torch.zeros(1, 3, device=device)\n",
    "        )\n",
    "    return output.joints[0].cpu().numpy()\n",
    "\n",
    "def solve_pnp(keypoints_2d, joints_3d, K):\n",
    "    \"\"\"Solve PnP for camera pose\"\"\"\n",
    "    pts_2d = []\n",
    "    pts_3d = []\n",
    "    \n",
    "    for coco_idx, smplx_idx in COCO_TO_SMPLX.items():\n",
    "        conf = keypoints_2d[coco_idx, 2]\n",
    "        if conf > 0.5:\n",
    "            pts_2d.append(keypoints_2d[coco_idx, :2])\n",
    "            pts_3d.append(joints_3d[smplx_idx])\n",
    "    \n",
    "    if len(pts_2d) < 6:\n",
    "        return None, None, False\n",
    "    \n",
    "    pts_2d = np.array(pts_2d, dtype=np.float64)\n",
    "    pts_3d = np.array(pts_3d, dtype=np.float64)\n",
    "    \n",
    "    success, rvec, tvec, _ = cv2.solvePnPRansac(\n",
    "        pts_3d, pts_2d, K, None,\n",
    "        iterationsCount=100,\n",
    "        reprojectionError=8.0\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        return None, None, False\n",
    "    \n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "    return R, tvec.flatten(), True\n",
    "\n",
    "# Camera intrinsics\n",
    "focal = max(VIDEO_INFO['width'], VIDEO_INFO['height'])\n",
    "K = np.array([\n",
    "    [focal, 0, VIDEO_INFO['width'] / 2],\n",
    "    [0, focal, VIDEO_INFO['height'] / 2],\n",
    "    [0, 0, 1]\n",
    "], dtype=np.float64)\n",
    "\n",
    "# Iterative PnP: 3 rounds\n",
    "print(\"üì∑ Iterative PnP Camera Estimation (3 rounds)...\")\n",
    "\n",
    "betas = torch.zeros(1, 10, device=device)\n",
    "cameras = [None] * len(pose_results)\n",
    "\n",
    "for iteration in range(3):\n",
    "    print(f\"\\n   Round {iteration + 1}/3:\")\n",
    "    \n",
    "    # Get 3D joints with current shape\n",
    "    joints_3d = get_smplx_joints(body_model, betas, device)\n",
    "    \n",
    "    # Estimate camera for each frame\n",
    "    success_count = 0\n",
    "    for i, pose in enumerate(pose_results):\n",
    "        if pose is None:\n",
    "            continue\n",
    "        \n",
    "        R, t, success = solve_pnp(pose['keypoints'], joints_3d, K)\n",
    "        \n",
    "        if success:\n",
    "            cameras[i] = {'R': R, 't': t, 'K': K.copy()}\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"      PnP success: {success_count}/{len([p for p in pose_results if p])}\")\n",
    "    \n",
    "    # Quick shape update (except last iteration)\n",
    "    if iteration < 2:\n",
    "        betas_opt = betas.clone().detach().requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([betas_opt], lr=0.05)\n",
    "        \n",
    "        for _ in range(30):\n",
    "            optimizer.zero_grad()\n",
    "            output = body_model(\n",
    "                betas=betas_opt,\n",
    "                body_pose=torch.zeros(1, 63, device=device),\n",
    "                global_orient=torch.zeros(1, 3, device=device)\n",
    "            )\n",
    "            joints = output.joints[0]\n",
    "            \n",
    "            loss = 0\n",
    "            for pose, cam in zip(pose_results, cameras):\n",
    "                if pose is None or cam is None:\n",
    "                    continue\n",
    "                # Simple reprojection loss\n",
    "                R_t = torch.tensor(cam['R'], dtype=torch.float32, device=device)\n",
    "                t_t = torch.tensor(cam['t'], dtype=torch.float32, device=device)\n",
    "                K_t = torch.tensor(cam['K'], dtype=torch.float32, device=device)\n",
    "                \n",
    "                body_joints = torch.stack([joints[COCO_TO_SMPLX[i]] for i in COCO_TO_SMPLX.keys()])\n",
    "                cam_pts = torch.matmul(body_joints, R_t.T) + t_t\n",
    "                proj = torch.matmul(cam_pts, K_t.T)\n",
    "                proj_2d = proj[:, :2] / (proj[:, 2:3] + 1e-8)\n",
    "                \n",
    "                gt_2d = torch.tensor(\n",
    "                    pose['keypoints'][list(COCO_TO_SMPLX.keys()), :2],\n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "                loss += torch.mean((proj_2d - gt_2d) ** 2)\n",
    "            \n",
    "            loss += 0.01 * torch.mean(betas_opt ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        betas = betas_opt.detach()\n",
    "        print(f\"      Shape updated (Œ≤0={betas[0,0].item():.3f})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Camera estimation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 6: Final Shape Optimization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Full Shape Optimization with Priors\n",
    "#@markdown Optimizes body shape using all views with symmetry constraints\n",
    "\n",
    "N_ITERATIONS = 200  #@param {type:\"slider\", min:100, max:500, step:50}\n",
    "\n",
    "# Initialize with refined betas from PnP\n",
    "betas_final = betas.clone().detach().requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([betas_final], lr=0.02)\n",
    "\n",
    "print(f\"üîß Final shape optimization ({N_ITERATIONS} iterations)...\")\n",
    "\n",
    "losses_history = []\n",
    "\n",
    "for iteration in range(N_ITERATIONS):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = body_model(\n",
    "        betas=betas_final,\n",
    "        body_pose=torch.zeros(1, 63, device=device),\n",
    "        global_orient=torch.zeros(1, 3, device=device)\n",
    "    )\n",
    "    joints = output.joints[0]\n",
    "    \n",
    "    # Keypoint reprojection loss\n",
    "    kp_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for pose, cam in zip(pose_results, cameras):\n",
    "        if pose is None or cam is None:\n",
    "            continue\n",
    "        \n",
    "        R_t = torch.tensor(cam['R'], dtype=torch.float32, device=device)\n",
    "        t_t = torch.tensor(cam['t'], dtype=torch.float32, device=device)\n",
    "        K_t = torch.tensor(cam['K'], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Project SMPL-X joints\n",
    "        body_joints = torch.stack([joints[COCO_TO_SMPLX[i]] for i in COCO_TO_SMPLX.keys()])\n",
    "        cam_pts = torch.matmul(body_joints, R_t.T) + t_t\n",
    "        proj = torch.matmul(cam_pts, K_t.T)\n",
    "        proj_2d = proj[:, :2] / (proj[:, 2:3] + 1e-8)\n",
    "        \n",
    "        # Ground truth\n",
    "        gt_2d = torch.tensor(\n",
    "            pose['keypoints'][list(COCO_TO_SMPLX.keys()), :2],\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        conf = torch.tensor(\n",
    "            pose['keypoints'][list(COCO_TO_SMPLX.keys()), 2],\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        diff = proj_2d - gt_2d\n",
    "        kp_loss += torch.sum(conf.unsqueeze(-1) * diff ** 2)\n",
    "        count += 1\n",
    "    \n",
    "    kp_loss = kp_loss / (count + 1e-8)\n",
    "    \n",
    "    # Symmetry loss (left arm ‚âà right arm, left leg ‚âà right leg)\n",
    "    left_arm = torch.norm(joints[16] - joints[18]) + torch.norm(joints[18] - joints[20])\n",
    "    right_arm = torch.norm(joints[17] - joints[19]) + torch.norm(joints[19] - joints[21])\n",
    "    left_leg = torch.norm(joints[1] - joints[4]) + torch.norm(joints[4] - joints[7])\n",
    "    right_leg = torch.norm(joints[2] - joints[5]) + torch.norm(joints[5] - joints[8])\n",
    "    \n",
    "    symmetry_loss = (left_arm - right_arm) ** 2 + (left_leg - right_leg) ** 2\n",
    "    \n",
    "    # Shape regularization\n",
    "    shape_loss = torch.mean(betas_final ** 2)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = kp_loss + 0.1 * symmetry_loss + 0.01 * shape_loss\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses_history.append(total_loss.item())\n",
    "    \n",
    "    if iteration % 50 == 0:\n",
    "        print(f\"   Iter {iteration}: Loss = {total_loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization complete (Final loss: {total_loss.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.2 Plot Optimization Progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Shape Optimization Progress')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 7: Extract Body Measurements\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.1 Generate Canonical Body Mesh\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = body_model(\n",
    "        betas=betas_final,\n",
    "        body_pose=torch.zeros(1, 63, device=device),\n",
    "        global_orient=torch.zeros(1, 3, device=device),\n",
    "        return_verts=True\n",
    "    )\n",
    "\n",
    "vertices = output.vertices[0].cpu().numpy()\n",
    "joints = output.joints[0].cpu().numpy()\n",
    "faces = body_model.faces\n",
    "\n",
    "print(f\"‚úÖ Canonical mesh generated\")\n",
    "print(f\"   Vertices: {vertices.shape}\")\n",
    "print(f\"   Joints: {joints.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Extract Measurements (PCA Circumferences)\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def measure_circumference_pca(vertices, center, radius=0.1, scale=100):\n",
    "    \"\"\"Measure circumference using PCA plane\"\"\"\n",
    "    distances = np.linalg.norm(vertices - center, axis=1)\n",
    "    nearby = vertices[distances < radius]\n",
    "    \n",
    "    if len(nearby) < 20:\n",
    "        radius *= 1.5\n",
    "        nearby = vertices[distances < radius]\n",
    "    \n",
    "    if len(nearby) < 10:\n",
    "        return 0.0\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    points_2d = pca.fit_transform(nearby - center)\n",
    "    \n",
    "    try:\n",
    "        hull = ConvexHull(points_2d)\n",
    "        hull_pts = points_2d[hull.vertices]\n",
    "        \n",
    "        perimeter = 0\n",
    "        for i in range(len(hull_pts)):\n",
    "            perimeter += np.linalg.norm(hull_pts[i] - hull_pts[(i+1) % len(hull_pts)])\n",
    "        \n",
    "        return perimeter * scale\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Known height for calibration (optional)\n",
    "KNOWN_HEIGHT_CM = None  #@param {type:\"number\"}\n",
    "\n",
    "# Scale factor\n",
    "raw_height = vertices[:, 1].max() - vertices[:, 1].min()\n",
    "scale = KNOWN_HEIGHT_CM / raw_height if KNOWN_HEIGHT_CM else 100\n",
    "\n",
    "# Extract measurements\n",
    "measurements = {}\n",
    "\n",
    "# Linear measurements\n",
    "measurements['height'] = raw_height * scale\n",
    "measurements['shoulder_width'] = np.linalg.norm(joints[16] - joints[17]) * scale\n",
    "measurements['hip_width'] = np.linalg.norm(joints[1] - joints[2]) * scale\n",
    "measurements['torso_length'] = np.linalg.norm(joints[12] - joints[0]) * scale\n",
    "\n",
    "# Arm length (averaged)\n",
    "left_arm = np.linalg.norm(joints[16] - joints[18]) + np.linalg.norm(joints[18] - joints[20])\n",
    "right_arm = np.linalg.norm(joints[17] - joints[19]) + np.linalg.norm(joints[19] - joints[21])\n",
    "measurements['arm_length'] = ((left_arm + right_arm) / 2) * scale\n",
    "\n",
    "# Leg length (averaged)\n",
    "left_leg = np.linalg.norm(joints[1] - joints[4]) + np.linalg.norm(joints[4] - joints[7])\n",
    "right_leg = np.linalg.norm(joints[2] - joints[5]) + np.linalg.norm(joints[5] - joints[8])\n",
    "measurements['leg_length'] = ((left_leg + right_leg) / 2) * scale\n",
    "\n",
    "# Inseam\n",
    "crotch = (joints[1] + joints[2]) / 2\n",
    "crotch[1] -= 0.03\n",
    "ankle = (joints[7] + joints[8]) / 2\n",
    "measurements['inseam'] = np.linalg.norm(crotch - ankle) * scale\n",
    "\n",
    "# Circumferences (PCA-based)\n",
    "chest_center = (joints[16] + joints[17]) / 2\n",
    "chest_center[1] -= 0.05\n",
    "measurements['chest_circumference'] = measure_circumference_pca(vertices, chest_center, 0.12, scale)\n",
    "\n",
    "waist_center = (joints[3] + joints[6]) / 2\n",
    "measurements['waist_circumference'] = measure_circumference_pca(vertices, waist_center, 0.10, scale)\n",
    "\n",
    "hip_center = joints[0].copy()\n",
    "measurements['hip_circumference'] = measure_circumference_pca(vertices, hip_center, 0.12, scale)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìè BODY MEASUREMENTS\")\n",
    "print(\"=\"*60)\n",
    "for name, value in measurements.items():\n",
    "    print(f\"   {name.replace('_', ' ').title():<25} {value:>8.1f} cm\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.3 Visualize Body Model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Front view\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(vertices[::10, 0], vertices[::10, 2], vertices[::10, 1], \n",
    "           c='lightblue', s=1, alpha=0.5)\n",
    "ax1.scatter(joints[:22, 0], joints[:22, 2], joints[:22, 1], c='red', s=50)\n",
    "ax1.set_title('Front View')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Z')\n",
    "ax1.set_zlabel('Y')\n",
    "\n",
    "# Side view\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(vertices[::10, 2], vertices[::10, 0], vertices[::10, 1],\n",
    "           c='lightblue', s=1, alpha=0.5)\n",
    "ax2.scatter(joints[:22, 2], joints[:22, 0], joints[:22, 1], c='red', s=50)\n",
    "ax2.set_title('Side View')\n",
    "ax2.view_init(elev=0, azim=0)\n",
    "\n",
    "# Measurements\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.axis('off')\n",
    "text = \"üìè MEASUREMENTS\\n\" + \"=\"*30 + \"\\n\\n\"\n",
    "for name, value in measurements.items():\n",
    "    text += f\"{name.replace('_', ' ').title()}: {value:.1f} cm\\n\"\n",
    "ax3.text(0.1, 0.9, text, transform=ax3.transAxes, fontsize=12,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('workspace/output/final/body_measurements.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 8: Export Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8.1 Save Results\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Save measurements JSON\n",
    "output_data = {\n",
    "    'measurements_cm': {k: float(v) for k, v in measurements.items()},\n",
    "    'pipeline_version': '2.1-fixed',\n",
    "    'betas': betas_final.cpu().numpy().tolist()\n",
    "}\n",
    "\n",
    "with open('workspace/output/final/measurements.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "# Save mesh OBJ\n",
    "with open('workspace/output/final/body.obj', 'w') as f:\n",
    "    for v in vertices:\n",
    "        f.write(f\"v {v[0]} {v[1]} {v[2]}\\n\")\n",
    "    for face in faces:\n",
    "        f.write(f\"f {face[0]+1} {face[1]+1} {face[2]+1}\\n\")\n",
    "\n",
    "# Save numpy data\n",
    "np.savez('workspace/output/final/body_data.npz',\n",
    "         vertices=vertices, joints=joints,\n",
    "         betas=betas_final.cpu().numpy())\n",
    "\n",
    "print(\"‚úÖ Results saved!\")\n",
    "print(\"   - measurements.json\")\n",
    "print(\"   - body.obj\")\n",
    "print(\"   - body_data.npz\")\n",
    "print(\"   - body_measurements.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8.2 Download Results\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive('body_reconstruction_results', 'zip', 'workspace/output/final')\n",
    "\n",
    "# Download\n",
    "files.download('body_reconstruction_results.zip')\n",
    "\n",
    "print(\"\\nüì• Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Complete!\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "You now have:\n",
    "- **measurements.json** - All body measurements in cm\n",
    "- **body.obj** - 3D mesh in T-pose (can view in Blender, MeshLab, etc.)\n",
    "- **body_data.npz** - Raw numpy data for further processing\n",
    "- **body_measurements.png** - Visualization\n",
    "\n",
    "## Expected Accuracy\n",
    "\n",
    "| Measurement | Expected Error |\n",
    "|-------------|----------------|\n",
    "| Height | ¬±1.0-1.5 cm |\n",
    "| Chest/Waist/Hip | ¬±2-3 cm |\n",
    "| Inseam | ¬±1.5-2 cm |\n",
    "| Arm/Leg length | ¬±1.5 cm |\n",
    "\n",
    "## Tips for Better Results\n",
    "\n",
    "1. **Known height**: If you know the person's height, set `KNOWN_HEIGHT_CM` for calibration\n",
    "2. **Better video**: Slow rotation, good lighting, tight clothing\n",
    "3. **More frames**: Increase `N_FRAMES` to 12-16 for complex poses"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
