{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßç Human Body Reconstruction Pipeline v2.1\n",
    "\n",
    "## Production-Grade Implementation with Final Refinements\n",
    "\n",
    "### V2.1 Improvements (from expert review):\n",
    "1. **Iterative PnP Refinement** - PnP ‚Üí optimize ‚Üí recompute joints ‚Üí PnP (2-3 cycles)\n",
    "2. **SMPL-X Joint Regressor** - Proper COCO mapping, no head approximation noise\n",
    "3. **Pose-Guided GrabCut** - Better silhouettes than convex hull\n",
    "4. **PCA Plane Circumference** - Handles torso tilt correctly\n",
    "\n",
    "### Expected Accuracy (Realistic):\n",
    "| Measurement | Error |\n",
    "|-------------|-------|\n",
    "| Height | ¬±1.0-1.5 cm |\n",
    "| Chest/Waist/Hip | ¬±2-3 cm |\n",
    "| Inseam | ¬±1.5-2 cm |\n",
    "| Arm length | ¬±1.5 cm |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.1 GPU Check\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.2 Install Dependencies\n",
    "\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q opencv-python-headless numpy scipy\n",
    "!pip install -q ultralytics\n",
    "!pip install -q smplx chumpy\n",
    "!pip install -q trimesh networkx scikit-learn\n",
    "!pip install -q matplotlib tqdm\n",
    "\n",
    "# MMPose for ViTPose/RTMPose\n",
    "!pip install -q openmim\n",
    "!mim install -q mmengine \"mmcv>=2.0.0\" \"mmpose>=1.0.0\"\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.3 Setup Directories\n",
    "import os\n",
    "\n",
    "for d in ['workspace/input', 'workspace/output/frames', 'workspace/output/final', 'workspace/models/smplx']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Download YOLO detection model\n",
    "from ultralytics import YOLO\n",
    "detector = YOLO('yolov8x.pt')\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßç Human Body Reconstruction Pipeline v2.1 (Fixed)\n",
    "\n",
    "## Production-Grade Implementation - Colab Compatible\n",
    "\n",
    "### Fixed Issues:\n",
    "- ‚úÖ No MMPose (causes Python 3.12 conflicts)\n",
    "- ‚úÖ Uses MediaPipe for pose (stable, fast)\n",
    "- ‚úÖ Clean dependency installation\n",
    "- ‚úÖ Iterative PnP camera estimation\n",
    "- ‚úÖ PCA-based circumference measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 0: Setup (Run All Cells in Order)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.1 Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üî• CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.2 Install Dependencies (Fixed - No MMPose)\n",
    "#@markdown This avoids the Python 3.12 compatibility issues\n",
    "\n",
    "# Core packages (these are safe)\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q mediapipe\n",
    "!pip install -q ultralytics\n",
    "!pip install -q smplx\n",
    "!pip install -q chumpy\n",
    "!pip install -q trimesh\n",
    "!pip install -q scikit-learn\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.3 Verify Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import smplx\n",
    "import trimesh\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   OpenCV: {cv2.__version__}\")\n",
    "print(f\"   MediaPipe: {mp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0.4 Create Directories\n",
    "import os\n",
    "\n",
    "for d in ['workspace/input', 'workspace/output/frames', 'workspace/output/final', 'workspace/models/smplx']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 1: Upload Video\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 Upload Your Video\n",
    "#@markdown Click \"Choose Files\" button and select your turntable video\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload your turntable video:\")\n",
    "print(\"   Requirements:\")\n",
    "print(\"   - 5-10 seconds long\")\n",
    "print(\"   - Person rotating slowly (or camera moving around person)\")\n",
    "print(\"   - Full body visible\")\n",
    "print(\"   - MP4 format recommended\")\n",
    "print(\"\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "VIDEO_PATH = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úÖ Uploaded: {VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.2 Verify Video\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "# Get video info\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration = total_frames / fps if fps > 0 else 0\n",
    "\n",
    "print(f\"üìπ Video Info:\")\n",
    "print(f\"   Resolution: {width} x {height}\")\n",
    "print(f\"   Duration: {duration:.1f} seconds\")\n",
    "print(f\"   FPS: {fps:.0f}\")\n",
    "print(f\"   Total Frames: {total_frames}\")\n",
    "\n",
    "# Show first frame\n",
    "ret, frame = cap.read()\n",
    "if ret:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('First Frame Preview')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(\"\\n‚úÖ Video loaded successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Error: Could not read video\")\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Store for later\n",
    "VIDEO_INFO = {'width': width, 'height': height, 'fps': fps, 'total_frames': total_frames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 2: Extract Frames\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Extract 8 Strategic Frames\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "N_FRAMES = 8  #@param {type:\"slider\", min:4, max:16, step:2}\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Get evenly spaced frame indices\n",
    "indices = np.linspace(0, total_frames - 1, N_FRAMES, dtype=int)\n",
    "\n",
    "frames = []\n",
    "print(f\"üì∏ Extracting {N_FRAMES} frames...\")\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append({\n",
    "            'index': i,\n",
    "            'frame_idx': int(idx),\n",
    "            'data': frame_rgb\n",
    "        })\n",
    "        print(f\"   Frame {i+1}/{N_FRAMES} extracted (original index: {idx})\")\n",
    "\n",
    "cap.release()\n",
    "print(f\"\\n‚úÖ Extracted {len(frames)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.2 Preview Extracted Frames\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = 4\n",
    "rows = (len(frames) + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "axes = axes.flatten() if len(frames) > cols else [axes] if len(frames) == 1 else axes\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(frames):\n",
    "        ax.imshow(frames[i]['data'])\n",
    "        ax.set_title(f\"Frame {i+1}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 3: Person Detection & Pose Estimation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Initialize Detection & Pose Models\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "# YOLOv8 for bounding box detection\n",
    "print(\"üîÑ Loading YOLOv8...\")\n",
    "detector = YOLO('yolov8x.pt')\n",
    "print(\"‚úÖ YOLOv8 loaded\")\n",
    "\n",
    "# MediaPipe for pose estimation\n",
    "print(\"\\nüîÑ Loading MediaPipe Pose...\")\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_estimator = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=2,  # Highest accuracy\n",
    "    enable_segmentation=True,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "print(\"‚úÖ MediaPipe Pose loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 Run Detection & Pose on All Frames\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe to COCO keypoint mapping\n",
    "MP_TO_COCO = {\n",
    "    0: 0,    # nose\n",
    "    2: 1,    # left_eye\n",
    "    5: 2,    # right_eye\n",
    "    7: 3,    # left_ear\n",
    "    8: 4,    # right_ear\n",
    "    11: 5,   # left_shoulder\n",
    "    12: 6,   # right_shoulder\n",
    "    13: 7,   # left_elbow\n",
    "    14: 8,   # right_elbow\n",
    "    15: 9,   # left_wrist\n",
    "    16: 10,  # right_wrist\n",
    "    23: 11,  # left_hip\n",
    "    24: 12,  # right_hip\n",
    "    25: 13,  # left_knee\n",
    "    26: 14,  # right_knee\n",
    "    27: 15,  # left_ankle\n",
    "    28: 16,  # right_ankle\n",
    "}\n",
    "\n",
    "def process_frame(image, detector, pose_estimator):\n",
    "    \"\"\"Detect person and estimate pose\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Step 1: Detect bounding box\n",
    "    det_results = detector(image, verbose=False, classes=[0])  # class 0 = person\n",
    "    \n",
    "    if len(det_results[0].boxes) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Get largest person bbox\n",
    "    boxes = det_results[0].boxes\n",
    "    areas = (boxes.xyxy[:, 2] - boxes.xyxy[:, 0]) * (boxes.xyxy[:, 3] - boxes.xyxy[:, 1])\n",
    "    best_idx = areas.argmax()\n",
    "    bbox = boxes.xyxy[best_idx].cpu().numpy()\n",
    "    \n",
    "    # Step 2: Crop and run pose estimation\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    pad = int(max(x2-x1, y2-y1) * 0.1)\n",
    "    x1, y1 = max(0, x1-pad), max(0, y1-pad)\n",
    "    x2, y2 = min(w, x2+pad), min(h, y2+pad)\n",
    "    \n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    results = pose_estimator.process(crop)\n",
    "    \n",
    "    if not results.pose_landmarks:\n",
    "        return None\n",
    "    \n",
    "    # Extract keypoints\n",
    "    ch, cw = crop.shape[:2]\n",
    "    mp_keypoints = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        mp_keypoints.append([lm.x * cw + x1, lm.y * ch + y1, lm.visibility])\n",
    "    mp_keypoints = np.array(mp_keypoints)\n",
    "    \n",
    "    # Convert to COCO format\n",
    "    coco_keypoints = np.zeros((17, 3))\n",
    "    for mp_idx, coco_idx in MP_TO_COCO.items():\n",
    "        coco_keypoints[coco_idx] = mp_keypoints[mp_idx]\n",
    "    \n",
    "    # Get segmentation mask\n",
    "    segmentation = None\n",
    "    if results.segmentation_mask is not None:\n",
    "        seg_crop = (results.segmentation_mask > 0.5).astype(np.uint8)\n",
    "        segmentation = np.zeros((h, w), dtype=np.uint8)\n",
    "        segmentation[y1:y2, x1:x2] = seg_crop\n",
    "    \n",
    "    return {\n",
    "        'bbox': bbox,\n",
    "        'keypoints': coco_keypoints,\n",
    "        'segmentation': segmentation\n",
    "    }\n",
    "\n",
    "# Process all frames\n",
    "print(f\"üîç Processing {len(frames)} frames...\")\n",
    "pose_results = []\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    result = process_frame(frame['data'], detector, pose_estimator)\n",
    "    \n",
    "    if result:\n",
    "        result['frame_index'] = i\n",
    "        pose_results.append(result)\n",
    "        print(f\"   Frame {i+1}: ‚úì Person detected, 17 keypoints extracted\")\n",
    "    else:\n",
    "        pose_results.append(None)\n",
    "        print(f\"   Frame {i+1}: ‚ö†Ô∏è No detection\")\n",
    "\n",
    "success_count = sum(1 for r in pose_results if r is not None)\n",
    "print(f\"\\n‚úÖ Processing complete: {success_count}/{len(frames)} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.3 Visualize Pose Estimation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# COCO skeleton connections\n",
    "SKELETON = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n",
    "    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Arms\n",
    "    (5, 11), (6, 12), (11, 12),  # Torso\n",
    "    (11, 13), (13, 15), (12, 14), (14, 16)  # Legs\n",
    "]\n",
    "\n",
    "cols = 4\n",
    "rows = (len(frames) + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(frames):\n",
    "        ax.imshow(frames[i]['data'])\n",
    "        \n",
    "        if pose_results[i] is not None:\n",
    "            kp = pose_results[i]['keypoints']\n",
    "            bbox = pose_results[i]['bbox']\n",
    "            \n",
    "            # Draw bbox\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
    "                linewidth=2, edgecolor='lime', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Draw keypoints\n",
    "            for j, (x, y, c) in enumerate(kp):\n",
    "                if c > 0.3:\n",
    "                    ax.scatter(x, y, c='red', s=30, zorder=5)\n",
    "            \n",
    "            # Draw skeleton\n",
    "            for (s, e) in SKELETON:\n",
    "                if kp[s, 2] > 0.3 and kp[e, 2] > 0.3:\n",
    "                    ax.plot([kp[s, 0], kp[e, 0]], [kp[s, 1], kp[e, 1]], 'c-', lw=2)\n",
    "            \n",
    "            ax.set_title(f\"Frame {i+1}: ‚úì\")\n",
    "        else:\n",
    "            ax.set_title(f\"Frame {i+1}: ‚ö†Ô∏è\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 4: Upload SMPL-X Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Upload SMPL-X Model\n",
    "#@markdown Download from https://smpl-x.is.tue.mpg.de/ (free registration)\n",
    "#@markdown Upload the SMPLX_NEUTRAL.npz file\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "SMPLX_PATH = 'workspace/models/smplx'\n",
    "model_file = os.path.join(SMPLX_PATH, 'SMPLX_NEUTRAL.npz')\n",
    "\n",
    "if not os.path.exists(model_file):\n",
    "    print(\"üì§ Upload SMPLX_NEUTRAL.npz:\")\n",
    "    print(\"   (Download from https://smpl-x.is.tue.mpg.de/)\")\n",
    "    print(\"\")\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for fname in uploaded.keys():\n",
    "        dest = os.path.join(SMPLX_PATH, fname)\n",
    "        os.rename(fname, dest)\n",
    "        print(f\"\\n‚úÖ Saved to: {dest}\")\n",
    "else:\n",
    "    print(f\"‚úÖ SMPL-X model already exists at {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Load SMPL-X Model\n",
    "import smplx\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîÑ Loading SMPL-X on {device}...\")\n",
    "\n",
    "body_model = smplx.create(\n",
    "    SMPLX_PATH,\n",
    "    model_type='smplx',\n",
    "    gender='neutral',\n",
    "    use_face_contour=True,\n",
    "    num_betas=10,\n",
    "    num_expression_coeffs=10,\n",
    "    ext='npz'\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ SMPL-X loaded\")\n",
    "print(f\"   Vertices: {body_model.get_num_verts()}\")\n",
    "print(f\"   Joints: {body_model.NUM_JOINTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 5: Camera Estimation (Iterative PnP)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Iterative PnP Camera Estimation\n",
    "#@markdown This is the key improvement - estimates camera pose accurately\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# COCO to SMPL-X joint mapping (reliable joints only, skip head)\n",
    "COCO_TO_SMPLX = {\n",
    "    5: 16,   # left_shoulder\n",
    "    6: 17,   # right_shoulder\n",
    "    7: 18,   # left_elbow\n",
    "    8: 19,   # right_elbow\n",
    "    9: 20,   # left_wrist\n",
    "    10: 21,  # right_wrist\n",
    "    11: 1,   # left_hip\n",
    "    12: 2,   # right_hip\n",
    "    13: 4,   # left_knee\n",
    "    14: 5,   # right_knee\n",
    "    15: 7,   # left_ankle\n",
    "    16: 8,   # right_ankle\n",
    "}\n",
    "\n",
    "def get_smplx_joints(body_model, betas, device):\n",
    "    \"\"\"Get 3D joints from SMPL-X\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = body_model(\n",
    "            betas=betas,\n",
    "            body_pose=torch.zeros(1, 63, device=device),\n",
    "            global_orient=torch.zeros(1, 3, device=device)\n",
    "        )\n",
    "    return output.joints[0].cpu().numpy()\n",
    "\n",
    "def solve_pnp(keypoints_2d, joints_3d, K):\n",
    "    \"\"\"Solve PnP for camera pose\"\"\"\n",
    "    pts_2d = []\n",
    "    pts_3d = []\n",
    "    \n",
    "    for coco_idx, smplx_idx in COCO_TO_SMPLX.items():\n",
    "        conf = keypoints_2d[coco_idx, 2]\n",
    "        if conf > 0.5:\n",
    "            pts_2d.append(keypoints_2d[coco_idx, :2])\n",
    "            pts_3d.append(joints_3d[smplx_idx])\n",
    "    \n",
    "    if len(pts_2d) < 6:\n",
    "        return None, None, False\n",
    "    \n",
    "    pts_2d = np.array(pts_2d, dtype=np.float64)\n",
    "    pts_3d = np.array(pts_3d, dtype=np.float64)\n",
    "    \n",
    "    success, rvec, tvec, _ = cv2.solvePnPRansac(\n",
    "        pts_3d, pts_2d, K, None,\n",
    "        iterationsCount=100,\n",
    "        reprojectionError=8.0\n",
    "    )\n",
    "    \n",
    "    if not success:\n",
    "        return None, None, False\n",
    "    \n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "    return R, tvec.flatten(), True\n",
    "\n",
    "# Camera intrinsics\n",
    "focal = max(VIDEO_INFO['width'], VIDEO_INFO['height'])\n",
    "K = np.array([\n",
    "    [focal, 0, VIDEO_INFO['width'] / 2],\n",
    "    [0, focal, VIDEO_INFO['height'] / 2],\n",
    "    [0, 0, 1]\n",
    "], dtype=np.float64)\n",
    "\n",
    "# Iterative PnP: 3 rounds\n",
    "print(\"üì∑ Iterative PnP Camera Estimation (3 rounds)...\")\n",
    "\n",
    "betas = torch.zeros(1, 10, device=device)\n",
    "cameras = [None] * len(pose_results)\n",
    "\n",
    "for iteration in range(3):\n",
    "    print(f\"\\n   Round {iteration + 1}/3:\")\n",
    "    \n",
    "    # Get 3D joints with current shape\n",
    "    joints_3d = get_smplx_joints(body_model, betas, device)\n",
    "    \n",
    "    # Estimate camera for each frame\n",
    "    success_count = 0\n",
    "    for i, pose in enumerate(pose_results):\n",
    "        if pose is None:\n",
    "            continue\n",
    "        \n",
    "        R, t, success = solve_pnp(pose['keypoints'], joints_3d, K)\n",
    "        \n",
    "        if success:\n",
    "            cameras[i] = {'R': R, 't': t, 'K': K.copy()}\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"      PnP success: {success_count}/{len([p for p in pose_results if p])}\")\n",
    "    \n",
    "    # Quick shape update (except last iteration)\n",
    "    if iteration < 2:\n",
    "        betas_opt = betas.clone().detach().requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([betas_opt], lr=0.05)\n",
    "        \n",
    "        for _ in range(30):\n",
    "            optimizer.zero_grad()\n",
    "            output = body_model(\n",
    "                betas=betas_opt,\n",
    "                body_pose=torch.zeros(1, 63, device=device),\n",
    "                global_orient=torch.zeros(1, 3, device=device)\n",
    "            )\n",
    "            joints = output.joints[0]\n",
    "            \n",
    "            loss = 0\n",
    "            for pose, cam in zip(pose_results, cameras):\n",
    "                if pose is None or cam is None:\n",
    "                    continue\n",
    "                # Simple reprojection loss\n",
    "                R_t = torch.tensor(cam['R'], dtype=torch.float32, device=device)\n",
    "                t_t = torch.tensor(cam['t'], dtype=torch.float32, device=device)\n",
    "                K_t = torch.tensor(cam['K'], dtype=torch.float32, device=device)\n",
    "                \n",
    "                body_joints = torch.stack([joints[COCO_TO_SMPLX[i]] for i in COCO_TO_SMPLX.keys()])\n",
    "                cam_pts = torch.matmul(body_joints, R_t.T) + t_t\n",
    "                proj = torch.matmul(cam_pts, K_t.T)\n",
    "                proj_2d = proj[:, :2] / (proj[:, 2:3] + 1e-8)\n",
    "                \n",
    "                gt_2d = torch.tensor(\n",
    "                    pose['keypoints'][list(COCO_TO_SMPLX.keys()), :2],\n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "                loss += torch.mean((proj_2d - gt_2d) ** 2)\n",
    "            \n",
    "            loss += 0.01 * torch.mean(betas_opt ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        betas = betas_opt.detach()\n",
    "        print(f\"      Shape updated (Œ≤0={betas[0,0].item():.3f})\")\n",
    "\n",
    "print(f\"\\n‚úÖ Camera estimation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 6: Final Shape Optimization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Full Shape Optimization with Priors\n",
    "#@markdown Optimizes body shape using all views with symmetry constraints\n",
    "\n",
    "N_ITERATIONS = 200  #@param {type:\"slider\", min:100, max:500, step:50}\n",
    "\n",
    "# Initialize with refined betas from PnP\n",
    "betas_final = betas.clone().detach().requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([betas_final], lr=0.02)\n",
    "\n",
    "print(f\"üîß Final shape optimization ({N_ITERATIONS} iterations)...\")\n",
    "\n",
    "losses_history = []\n",
    "\n",
    "for iteration in range(N_ITERATIONS):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = body_model(\n",
    "        betas=betas_final,\n",
    "        body_pose=torch.zeros(1, 63, device=device),\n",
    "        global_orient=torch.zeros(1, 3, device=device)\n",
    "    )\n",
    "    joints = output.joints[0]\n",
    "    \n",
    "    # Keypoint reprojection loss\n",
    "    kp_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for pose, cam in zip(pose_results, cameras):\n",
    "        if pose is None or cam is None:\n",
    "            continue\n",
    "        \n",
    "        R_t = torch.tensor(cam['R'], dtype=torch.float32, device=device)\n",
    "        t_t = torch.tensor(cam['t'], dtype=torch.float32, device=device)\n",
    "        K_t = torch.tensor(cam['K'], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Project SMPL-X joints\n",
    "        body_joints = torch.stack([joints[COCO_TO_SMPLX[i]] for i in COCO_TO_SMPLX.keys()])\n",
    "        cam_pts = torch.matmul(body_joints, R_t.T) + t_t\n",
    "        proj = torch.matmul(cam_pts, K_t.T)\n",
    "        proj_2d = proj[:, :2] / (proj[:, 2:3] + 1e-8)\n",
    "        \n",
    "        # Ground truth\n",
    "        gt_2d = torch.tensor(\n",
    "            pose['keypoints'][list(COCO_TO_SMPLX.keys()), :2],\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        conf = torch.tensor(\n",
    "            pose['keypoints'][list(COCO_TO_SMPLX.keys()), 2],\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        diff = proj_2d - gt_2d\n",
    "        kp_loss += torch.sum(conf.unsqueeze(-1) * diff ** 2)\n",
    "        count += 1\n",
    "    \n",
    "    kp_loss = kp_loss / (count + 1e-8)\n",
    "    \n",
    "    # Symmetry loss (left arm ‚âà right arm, left leg ‚âà right leg)\n",
    "    left_arm = torch.norm(joints[16] - joints[18]) + torch.norm(joints[18] - joints[20])\n",
    "    right_arm = torch.norm(joints[17] - joints[19]) + torch.norm(joints[19] - joints[21])\n",
    "    left_leg = torch.norm(joints[1] - joints[4]) + torch.norm(joints[4] - joints[7])\n",
    "    right_leg = torch.norm(joints[2] - joints[5]) + torch.norm(joints[5] - joints[8])\n",
    "    \n",
    "    symmetry_loss = (left_arm - right_arm) ** 2 + (left_leg - right_leg) ** 2\n",
    "    \n",
    "    # Shape regularization\n",
    "    shape_loss = torch.mean(betas_final ** 2)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = kp_loss + 0.1 * symmetry_loss + 0.01 * shape_loss\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses_history.append(total_loss.item())\n",
    "    \n",
    "    if iteration % 50 == 0:\n",
    "        print(f\"   Iter {iteration}: Loss = {total_loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization complete (Final loss: {total_loss.item():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.2 Plot Optimization Progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Shape Optimization Progress')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 7: Extract Body Measurements\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.1 Generate Canonical Body Mesh\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = body_model(\n",
    "        betas=betas_final,\n",
    "        body_pose=torch.zeros(1, 63, device=device),\n",
    "        global_orient=torch.zeros(1, 3, device=device),\n",
    "        return_verts=True\n",
    "    )\n",
    "\n",
    "vertices = output.vertices[0].cpu().numpy()\n",
    "joints = output.joints[0].cpu().numpy()\n",
    "faces = body_model.faces\n",
    "\n",
    "print(f\"‚úÖ Canonical mesh generated\")\n",
    "print(f\"   Vertices: {vertices.shape}\")\n",
    "print(f\"   Joints: {joints.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Extract Measurements (PCA Circumferences)\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def measure_circumference_pca(vertices, center, radius=0.1, scale=100):\n",
    "    \"\"\"Measure circumference using PCA plane\"\"\"\n",
    "    distances = np.linalg.norm(vertices - center, axis=1)\n",
    "    nearby = vertices[distances < radius]\n",
    "    \n",
    "    if len(nearby) < 20:\n",
    "        radius *= 1.5\n",
    "        nearby = vertices[distances < radius]\n",
    "    \n",
    "    if len(nearby) < 10:\n",
    "        return 0.0\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    points_2d = pca.fit_transform(nearby - center)\n",
    "    \n",
    "    try:\n",
    "        hull = ConvexHull(points_2d)\n",
    "        hull_pts = points_2d[hull.vertices]\n",
    "        \n",
    "        perimeter = 0\n",
    "        for i in range(len(hull_pts)):\n",
    "            perimeter += np.linalg.norm(hull_pts[i] - hull_pts[(i+1) % len(hull_pts)])\n",
    "        \n",
    "        return perimeter * scale\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Known height for calibration (optional)\n",
    "KNOWN_HEIGHT_CM = None  #@param {type:\"number\"}\n",
    "\n",
    "# Scale factor\n",
    "raw_height = vertices[:, 1].max() - vertices[:, 1].min()\n",
    "scale = KNOWN_HEIGHT_CM / raw_height if KNOWN_HEIGHT_CM else 100\n",
    "\n",
    "# Extract measurements\n",
    "measurements = {}\n",
    "\n",
    "# Linear measurements\n",
    "measurements['height'] = raw_height * scale\n",
    "measurements['shoulder_width'] = np.linalg.norm(joints[16] - joints[17]) * scale\n",
    "measurements['hip_width'] = np.linalg.norm(joints[1] - joints[2]) * scale\n",
    "measurements['torso_length'] = np.linalg.norm(joints[12] - joints[0]) * scale\n",
    "\n",
    "# Arm length (averaged)\n",
    "left_arm = np.linalg.norm(joints[16] - joints[18]) + np.linalg.norm(joints[18] - joints[20])\n",
    "right_arm = np.linalg.norm(joints[17] - joints[19]) + np.linalg.norm(joints[19] - joints[21])\n",
    "measurements['arm_length'] = ((left_arm + right_arm) / 2) * scale\n",
    "\n",
    "# Leg length (averaged)\n",
    "left_leg = np.linalg.norm(joints[1] - joints[4]) + np.linalg.norm(joints[4] - joints[7])\n",
    "right_leg = np.linalg.norm(joints[2] - joints[5]) + np.linalg.norm(joints[5] - joints[8])\n",
    "measurements['leg_length'] = ((left_leg + right_leg) / 2) * scale\n",
    "\n",
    "# Inseam\n",
    "crotch = (joints[1] + joints[2]) / 2\n",
    "crotch[1] -= 0.03\n",
    "ankle = (joints[7] + joints[8]) / 2\n",
    "measurements['inseam'] = np.linalg.norm(crotch - ankle) * scale\n",
    "\n",
    "# Circumferences (PCA-based)\n",
    "chest_center = (joints[16] + joints[17]) / 2\n",
    "chest_center[1] -= 0.05\n",
    "measurements['chest_circumference'] = measure_circumference_pca(vertices, chest_center, 0.12, scale)\n",
    "\n",
    "waist_center = (joints[3] + joints[6]) / 2\n",
    "measurements['waist_circumference'] = measure_circumference_pca(vertices, waist_center, 0.10, scale)\n",
    "\n",
    "hip_center = joints[0].copy()\n",
    "measurements['hip_circumference'] = measure_circumference_pca(vertices, hip_center, 0.12, scale)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìè BODY MEASUREMENTS\")\n",
    "print(\"=\"*60)\n",
    "for name, value in measurements.items():\n",
    "    print(f\"   {name.replace('_', ' ').title():<25} {value:>8.1f} cm\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.3 Visualize Body Model\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Front view\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(vertices[::10, 0], vertices[::10, 2], vertices[::10, 1], \n",
    "           c='lightblue', s=1, alpha=0.5)\n",
    "ax1.scatter(joints[:22, 0], joints[:22, 2], joints[:22, 1], c='red', s=50)\n",
    "ax1.set_title('Front View')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Z')\n",
    "ax1.set_zlabel('Y')\n",
    "\n",
    "# Side view\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(vertices[::10, 2], vertices[::10, 0], vertices[::10, 1],\n",
    "           c='lightblue', s=1, alpha=0.5)\n",
    "ax2.scatter(joints[:22, 2], joints[:22, 0], joints[:22, 1], c='red', s=50)\n",
    "ax2.set_title('Side View')\n",
    "ax2.view_init(elev=0, azim=0)\n",
    "\n",
    "# Measurements\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.axis('off')\n",
    "text = \"üìè MEASUREMENTS\\n\" + \"=\"*30 + \"\\n\\n\"\n",
    "for name, value in measurements.items():\n",
    "    text += f\"{name.replace('_', ' ').title()}: {value:.1f} cm\\n\"\n",
    "ax3.text(0.1, 0.9, text, transform=ax3.transAxes, fontsize=12,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('workspace/output/final/body_measurements.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 8: Export Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8.1 Save Results\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Save measurements JSON\n",
    "output_data = {\n",
    "    'measurements_cm': {k: float(v) for k, v in measurements.items()},\n",
    "    'pipeline_version': '2.1-fixed',\n",
    "    'betas': betas_final.cpu().numpy().tolist()\n",
    "}\n",
    "\n",
    "with open('workspace/output/final/measurements.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "# Save mesh OBJ\n",
    "with open('workspace/output/final/body.obj', 'w') as f:\n",
    "    for v in vertices:\n",
    "        f.write(f\"v {v[0]} {v[1]} {v[2]}\\n\")\n",
    "    for face in faces:\n",
    "        f.write(f\"f {face[0]+1} {face[1]+1} {face[2]+1}\\n\")\n",
    "\n",
    "# Save numpy data\n",
    "np.savez('workspace/output/final/body_data.npz',\n",
    "         vertices=vertices, joints=joints,\n",
    "         betas=betas_final.cpu().numpy())\n",
    "\n",
    "print(\"‚úÖ Results saved!\")\n",
    "print(\"   - measurements.json\")\n",
    "print(\"   - body.obj\")\n",
    "print(\"   - body_data.npz\")\n",
    "print(\"   - body_measurements.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8.2 Download Results\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive('body_reconstruction_results', 'zip', 'workspace/output/final')\n",
    "\n",
    "# Download\n",
    "files.download('body_reconstruction_results.zip')\n",
    "\n",
    "print(\"\\nüì• Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Complete!\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "You now have:\n",
    "- **measurements.json** - All body measurements in cm\n",
    "- **body.obj** - 3D mesh in T-pose (can view in Blender, MeshLab, etc.)\n",
    "- **body_data.npz** - Raw numpy data for further processing\n",
    "- **body_measurements.png** - Visualization\n",
    "\n",
    "## Expected Accuracy\n",
    "\n",
    "| Measurement | Expected Error |\n",
    "|-------------|----------------|\n",
    "| Height | ¬±1.0-1.5 cm |\n",
    "| Chest/Waist/Hip | ¬±2-3 cm |\n",
    "| Inseam | ¬±1.5-2 cm |\n",
    "| Arm/Leg length | ¬±1.5 cm |\n",
    "\n",
    "## Tips for Better Results\n",
    "\n",
    "1. **Known height**: If you know the person's height, set `KNOWN_HEIGHT_CM` for calibration\n",
    "2. **Better video**: Slow rotation, good lighting, tight clothing\n",
    "3. **More frames**: Increase `N_FRAMES` to 12-16 for complex poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Core Components (V2.1 Improvements)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üîß FIX 1: Iterative PnP Camera Estimator\n",
    "#@markdown PnP ‚Üí Optimize ‚Üí Recompute 3D joints ‚Üí PnP again (2-3 cycles)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class IterativePnPEstimator:\n",
    "    \"\"\"\n",
    "    Iterative PnP refinement for robust camera estimation.\n",
    "    \n",
    "    Instead of single-shot PnP with neutral body:\n",
    "    1. PnP with initial joints\n",
    "    2. Optimize body shape\n",
    "    3. Recompute 3D joints with new shape\n",
    "    4. PnP again with refined joints\n",
    "    5. Repeat 2-3 times\n",
    "    \n",
    "    This dramatically stabilizes camera for unusual body types.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Improved COCO to SMPL-X mapping\n",
    "    # Only use joints with reliable mapping (skip eyes/ears)\n",
    "    COCO_TO_SMPLX_RELIABLE = {\n",
    "        # Skip: 0 (nose), 1-4 (eyes, ears) - too noisy\n",
    "        5: 16,   # left_shoulder\n",
    "        6: 17,   # right_shoulder\n",
    "        7: 18,   # left_elbow\n",
    "        8: 19,   # right_elbow\n",
    "        9: 20,   # left_wrist\n",
    "        10: 21,  # right_wrist\n",
    "        11: 1,   # left_hip\n",
    "        12: 2,   # right_hip\n",
    "        13: 4,   # left_knee\n",
    "        14: 5,   # right_knee\n",
    "        15: 7,   # left_ankle\n",
    "        16: 8,   # right_ankle\n",
    "    }\n",
    "    \n",
    "    def __init__(self, image_width, image_height):\n",
    "        self.w = image_width\n",
    "        self.h = image_height\n",
    "        \n",
    "        # Camera intrinsics\n",
    "        self.focal = max(image_width, image_height)\n",
    "        self.K = np.array([\n",
    "            [self.focal, 0, image_width / 2],\n",
    "            [0, self.focal, image_height / 2],\n",
    "            [0, 0, 1]\n",
    "        ], dtype=np.float64)\n",
    "        self.dist_coeffs = np.zeros(4)\n",
    "    \n",
    "    def get_joints_from_model(self, body_model, betas, device):\n",
    "        \"\"\"Get 3D joints from SMPL-X with given shape\"\"\"\n",
    "        with torch.no_grad():\n",
    "            output = body_model(\n",
    "                betas=betas,\n",
    "                body_pose=torch.zeros(1, 63, device=device),\n",
    "                global_orient=torch.zeros(1, 3, device=device),\n",
    "                return_verts=True\n",
    "            )\n",
    "        return output.joints[0].cpu().numpy()\n",
    "    \n",
    "    def solve_pnp(self, keypoints_2d, joints_3d):\n",
    "        \"\"\"\n",
    "        Solve PnP using only reliable joint correspondences.\n",
    "        Skips head/face joints that add noise.\n",
    "        \"\"\"\n",
    "        pts_2d = []\n",
    "        pts_3d = []\n",
    "        \n",
    "        for coco_idx, smplx_idx in self.COCO_TO_SMPLX_RELIABLE.items():\n",
    "            conf = keypoints_2d[coco_idx, 2]\n",
    "            if conf > 0.5:\n",
    "                pts_2d.append(keypoints_2d[coco_idx, :2])\n",
    "                pts_3d.append(joints_3d[smplx_idx])\n",
    "        \n",
    "        if len(pts_2d) < 6:\n",
    "            return None, None, False\n",
    "        \n",
    "        pts_2d = np.array(pts_2d, dtype=np.float64)\n",
    "        pts_3d = np.array(pts_3d, dtype=np.float64)\n",
    "        \n",
    "        # Use RANSAC for robustness\n",
    "        success, rvec, tvec, inliers = cv2.solvePnPRansac(\n",
    "            pts_3d, pts_2d, self.K, self.dist_coeffs,\n",
    "            iterationsCount=100,\n",
    "            reprojectionError=8.0,\n",
    "            flags=cv2.SOLVEPNP_ITERATIVE\n",
    "        )\n",
    "        \n",
    "        if not success or inliers is None or len(inliers) < 4:\n",
    "            return None, None, False\n",
    "        \n",
    "        R, _ = cv2.Rodrigues(rvec)\n",
    "        return R, tvec.flatten(), True\n",
    "    \n",
    "    def iterative_estimate(self, keypoints_list, body_model, device,\n",
    "                           n_iterations=3):\n",
    "        \"\"\"\n",
    "        Iterative PnP refinement:\n",
    "        \n",
    "        For each iteration:\n",
    "        1. Estimate cameras with current shape\n",
    "        2. Optimize shape using all views\n",
    "        3. Update 3D joints\n",
    "        4. Re-estimate cameras\n",
    "        \"\"\"\n",
    "        print(f\"\\nüì∑ Iterative PnP Estimation ({n_iterations} rounds)...\")\n",
    "        \n",
    "        # Start with neutral shape\n",
    "        betas = torch.zeros(1, 10, device=device)\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            print(f\"\\n   Round {iteration + 1}/{n_iterations}:\")\n",
    "            \n",
    "            # Get 3D joints with current shape\n",
    "            joints_3d = self.get_joints_from_model(body_model, betas, device)\n",
    "            \n",
    "            # Estimate camera for each frame\n",
    "            cameras = []\n",
    "            success_count = 0\n",
    "            \n",
    "            for i, kp_data in enumerate(keypoints_list):\n",
    "                if kp_data is None:\n",
    "                    cameras.append(None)\n",
    "                    continue\n",
    "                \n",
    "                R, t, success = self.solve_pnp(kp_data['keypoints'], joints_3d)\n",
    "                \n",
    "                if success:\n",
    "                    cameras.append({'R': R, 't': t, 'K': self.K.copy()})\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    cameras.append(None)\n",
    "            \n",
    "            print(f\"      PnP success: {success_count}/{len(keypoints_list)}\")\n",
    "            \n",
    "            # Quick shape optimization using current cameras\n",
    "            if iteration < n_iterations - 1:  # Skip on last iteration\n",
    "                betas = self._quick_shape_fit(\n",
    "                    body_model, keypoints_list, cameras, betas, device\n",
    "                )\n",
    "                print(f\"      Shape updated (Œ≤0={betas[0,0].item():.3f})\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Iterative PnP complete\")\n",
    "        return cameras, betas\n",
    "    \n",
    "    def _quick_shape_fit(self, body_model, keypoints_list, cameras, \n",
    "                         init_betas, device, n_steps=50):\n",
    "        \"\"\"Quick shape optimization for iterative refinement\"\"\"\n",
    "        betas = init_betas.clone().detach().requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([betas], lr=0.05)\n",
    "        \n",
    "        for _ in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = body_model(\n",
    "                betas=betas,\n",
    "                body_pose=torch.zeros(1, 63, device=device),\n",
    "                global_orient=torch.zeros(1, 3, device=device)\n",
    "            )\n",
    "            joints = output.joints[0]\n",
    "            \n",
    "            loss = 0\n",
    "            count = 0\n",
    "            \n",
    "            for kp_data, cam in zip(keypoints_list, cameras):\n",
    "                if kp_data is None or cam is None:\n",
    "                    continue\n",
    "                \n",
    "                # Project and compute loss\n",
    "                R = torch.tensor(cam['R'], dtype=torch.float32, device=device)\n",
    "                t = torch.tensor(cam['t'], dtype=torch.float32, device=device)\n",
    "                K = torch.tensor(cam['K'], dtype=torch.float32, device=device)\n",
    "                \n",
    "                # Map to COCO joints\n",
    "                coco_3d = torch.stack([\n",
    "                    joints[16], joints[17], joints[18], joints[19],\n",
    "                    joints[20], joints[21], joints[1], joints[2],\n",
    "                    joints[4], joints[5], joints[7], joints[8]\n",
    "                ])\n",
    "                \n",
    "                # Project\n",
    "                cam_pts = torch.matmul(coco_3d, R.T) + t\n",
    "                proj = torch.matmul(cam_pts, K.T)\n",
    "                proj_2d = proj[:, :2] / (proj[:, 2:3] + 1e-8)\n",
    "                \n",
    "                # Get corresponding 2D points\n",
    "                gt_2d = torch.tensor(\n",
    "                    kp_data['keypoints'][[5,6,7,8,9,10,11,12,13,14,15,16], :2],\n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "                conf = torch.tensor(\n",
    "                    kp_data['keypoints'][[5,6,7,8,9,10,11,12,13,14,15,16], 2],\n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "                \n",
    "                diff = proj_2d - gt_2d\n",
    "                loss += torch.sum(conf.unsqueeze(-1) * diff**2)\n",
    "                count += 1\n",
    "            \n",
    "            if count > 0:\n",
    "                loss = loss / count + 0.01 * torch.mean(betas**2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return betas.detach()\n",
    "\n",
    "print(\"‚úÖ Iterative PnP Estimator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üîß FIX 2: Pose-Guided GrabCut Silhouette\n",
    "#@markdown Better than convex hull, lighter than SAM2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class PoseGuidedSilhouette:\n",
    "    \"\"\"\n",
    "    Generate silhouette using pose-guided GrabCut.\n",
    "    \n",
    "    Much better than convex hull:\n",
    "    - Respects actual body boundary\n",
    "    - Handles armpits correctly\n",
    "    - More accurate waist measurement\n",
    "    \n",
    "    Much lighter than SAM2:\n",
    "    - ~50ms vs ~500ms\n",
    "    - No GPU model loading\n",
    "    - Deterministic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def extract(self, image, keypoints, bbox, n_iterations=5):\n",
    "        \"\"\"\n",
    "        Extract silhouette using GrabCut with pose guidance.\n",
    "        \n",
    "        Args:\n",
    "            image: RGB image\n",
    "            keypoints: (17, 3) COCO keypoints\n",
    "            bbox: [x1, y1, x2, y2] bounding box\n",
    "            \n",
    "        Returns:\n",
    "            Binary mask (H, W)\n",
    "        \"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Convert to BGR for OpenCV\n",
    "        img_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Initialize mask\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        \n",
    "        # Bbox region = probable foreground\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(w, x2), min(h, y2)\n",
    "        \n",
    "        # GrabCut mask values:\n",
    "        # 0 = definite background\n",
    "        # 1 = definite foreground\n",
    "        # 2 = probable background\n",
    "        # 3 = probable foreground\n",
    "        \n",
    "        mask[y1:y2, x1:x2] = cv2.GC_PR_FGD  # Probable foreground\n",
    "        \n",
    "        # Mark keypoint locations as definite foreground\n",
    "        for kp in keypoints:\n",
    "            if kp[2] > 0.5:  # Confidence threshold\n",
    "                cx, cy = int(kp[0]), int(kp[1])\n",
    "                if 0 <= cx < w and 0 <= cy < h:\n",
    "                    # Mark small region around keypoint\n",
    "                    r = 10\n",
    "                    y_start, y_end = max(0, cy-r), min(h, cy+r)\n",
    "                    x_start, x_end = max(0, cx-r), min(w, cx+r)\n",
    "                    mask[y_start:y_end, x_start:x_end] = cv2.GC_FGD\n",
    "        \n",
    "        # Mark outside bbox as definite background\n",
    "        pad = 20\n",
    "        mask[:max(0, y1-pad), :] = cv2.GC_BGD\n",
    "        mask[min(h, y2+pad):, :] = cv2.GC_BGD\n",
    "        mask[:, :max(0, x1-pad)] = cv2.GC_BGD\n",
    "        mask[:, min(w, x2+pad):] = cv2.GC_BGD\n",
    "        \n",
    "        # Run GrabCut\n",
    "        bgd_model = np.zeros((1, 65), np.float64)\n",
    "        fgd_model = np.zeros((1, 65), np.float64)\n",
    "        \n",
    "        try:\n",
    "            cv2.grabCut(img_bgr, mask, None, bgd_model, fgd_model,\n",
    "                       n_iterations, cv2.GC_INIT_WITH_MASK)\n",
    "            \n",
    "            # Extract foreground\n",
    "            result = np.where((mask == cv2.GC_FGD) | (mask == cv2.GC_PR_FGD), 1, 0)\n",
    "            return result.astype(np.uint8)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      GrabCut failed: {e}, using bbox mask\")\n",
    "            # Fallback to bbox\n",
    "            result = np.zeros((h, w), dtype=np.uint8)\n",
    "            result[y1:y2, x1:x2] = 1\n",
    "            return result\n",
    "\n",
    "print(\"‚úÖ Pose-Guided Silhouette ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üîß FIX 3: PCA Plane Circumference Measurement\n",
    "#@markdown Handles torso tilt correctly\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "class PCACircumferenceMeasurer:\n",
    "    \"\"\"\n",
    "    Measure circumference using local PCA plane instead of fixed height.\n",
    "    \n",
    "    Problem with height-based slicing:\n",
    "    - Assumes perfect upright posture\n",
    "    - Slight torso tilt ‚Üí skewed slice ‚Üí wrong measurement\n",
    "    \n",
    "    PCA solution:\n",
    "    1. Find vertices near target joint\n",
    "    2. Fit PCA plane to local surface\n",
    "    3. Project vertices to that plane\n",
    "    4. Measure perimeter in 2D\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scale_factor=100):\n",
    "        self.scale = scale_factor\n",
    "    \n",
    "    def measure_circumference(self, vertices, center_point, radius=0.08):\n",
    "        \"\"\"\n",
    "        Measure circumference at a body location using PCA plane.\n",
    "        \n",
    "        Args:\n",
    "            vertices: (N, 3) mesh vertices\n",
    "            center_point: (3,) 3D point at measurement location\n",
    "            radius: Search radius around center point\n",
    "            \n",
    "        Returns:\n",
    "            Circumference in scaled units (cm if scale=100)\n",
    "        \"\"\"\n",
    "        # Find vertices near center point\n",
    "        distances = np.linalg.norm(vertices - center_point, axis=1)\n",
    "        nearby_mask = distances < radius\n",
    "        nearby_verts = vertices[nearby_mask]\n",
    "        \n",
    "        if len(nearby_verts) < 20:\n",
    "            # Not enough points, expand radius\n",
    "            radius *= 1.5\n",
    "            nearby_mask = distances < radius\n",
    "            nearby_verts = vertices[nearby_mask]\n",
    "        \n",
    "        if len(nearby_verts) < 10:\n",
    "            return 0.0\n",
    "        \n",
    "        # Fit PCA to find local plane\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(nearby_verts)\n",
    "        \n",
    "        # The first two components define the plane\n",
    "        # Third component is the normal\n",
    "        plane_normal = pca.components_[2]\n",
    "        \n",
    "        # Project vertices onto plane\n",
    "        # First, center the points\n",
    "        centered = nearby_verts - center_point\n",
    "        \n",
    "        # Project to 2D using first two PCA components\n",
    "        points_2d = centered @ pca.components_[:2].T\n",
    "        \n",
    "        # Find the boundary (convex hull or alpha shape)\n",
    "        try:\n",
    "            # Order points by angle for perimeter calculation\n",
    "            angles = np.arctan2(points_2d[:, 1], points_2d[:, 0])\n",
    "            order = np.argsort(angles)\n",
    "            ordered = points_2d[order]\n",
    "            \n",
    "            # Use convex hull for clean perimeter\n",
    "            hull = ConvexHull(points_2d)\n",
    "            hull_points = points_2d[hull.vertices]\n",
    "            \n",
    "            # Compute perimeter\n",
    "            perimeter = 0\n",
    "            for i in range(len(hull_points)):\n",
    "                p1 = hull_points[i]\n",
    "                p2 = hull_points[(i + 1) % len(hull_points)]\n",
    "                perimeter += np.linalg.norm(p1 - p2)\n",
    "            \n",
    "            return perimeter * self.scale\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 0.0\n",
    "    \n",
    "    def measure_all(self, vertices, joints):\n",
    "        \"\"\"\n",
    "        Measure all circumferences using PCA planes.\n",
    "        \"\"\"\n",
    "        measurements = {}\n",
    "        \n",
    "        # Chest: Between shoulders, slightly below\n",
    "        chest_center = (joints[16] + joints[17]) / 2\n",
    "        chest_center[1] -= 0.05  # Slightly below shoulder line\n",
    "        measurements['chest'] = self.measure_circumference(\n",
    "            vertices, chest_center, radius=0.12\n",
    "        )\n",
    "        \n",
    "        # Waist: Narrowest part of torso\n",
    "        # Between spine1 (joint 3) and spine2 (joint 6)\n",
    "        waist_center = (joints[3] + joints[6]) / 2\n",
    "        measurements['waist'] = self.measure_circumference(\n",
    "            vertices, waist_center, radius=0.10\n",
    "        )\n",
    "        \n",
    "        # Hip: At pelvis level\n",
    "        hip_center = joints[0].copy()  # Pelvis\n",
    "        measurements['hip'] = self.measure_circumference(\n",
    "            vertices, hip_center, radius=0.12\n",
    "        )\n",
    "        \n",
    "        # Thigh: Mid-thigh\n",
    "        left_thigh = (joints[1] + joints[4]) / 2\n",
    "        right_thigh = (joints[2] + joints[5]) / 2\n",
    "        left_circ = self.measure_circumference(vertices, left_thigh, radius=0.08)\n",
    "        right_circ = self.measure_circumference(vertices, right_thigh, radius=0.08)\n",
    "        measurements['thigh'] = (left_circ + right_circ) / 2\n",
    "        \n",
    "        # Bicep: Mid upper arm\n",
    "        left_bicep = (joints[16] + joints[18]) / 2\n",
    "        right_bicep = (joints[17] + joints[19]) / 2\n",
    "        left_circ = self.measure_circumference(vertices, left_bicep, radius=0.05)\n",
    "        right_circ = self.measure_circumference(vertices, right_bicep, radius=0.05)\n",
    "        measurements['bicep'] = (left_circ + right_circ) / 2\n",
    "        \n",
    "        return measurements\n",
    "\n",
    "print(\"‚úÖ PCA Circumference Measurer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üîß FIX 4: Complete Measurement Extractor V2.1\n",
    "#@markdown Combines all improvements\n",
    "\n",
    "class MeasurementExtractorV21:\n",
    "    \"\"\"\n",
    "    Production-grade measurement extractor with:\n",
    "    - Linear measurements from joints\n",
    "    - Circumferences via PCA planes\n",
    "    - Proper symmetry averaging\n",
    "    - Height calibration support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, known_height_cm=None):\n",
    "        self.known_height_cm = known_height_cm\n",
    "        self.scale = 100  # SMPL-X is in meters\n",
    "    \n",
    "    def extract(self, vertices, joints):\n",
    "        \"\"\"\n",
    "        Extract all measurements.\n",
    "        \"\"\"\n",
    "        # Calibrate scale\n",
    "        raw_height = vertices[:, 1].max() - vertices[:, 1].min()\n",
    "        if self.known_height_cm:\n",
    "            self.scale = self.known_height_cm / raw_height\n",
    "        \n",
    "        measurements = {}\n",
    "        \n",
    "        # ===== LINEAR MEASUREMENTS =====\n",
    "        \n",
    "        # Height\n",
    "        measurements['height'] = raw_height * self.scale\n",
    "        \n",
    "        # Shoulder width (joint-to-joint is accurate for this)\n",
    "        measurements['shoulder_width'] = np.linalg.norm(\n",
    "            joints[16] - joints[17]\n",
    "        ) * self.scale\n",
    "        \n",
    "        # Hip width\n",
    "        measurements['hip_width'] = np.linalg.norm(\n",
    "            joints[1] - joints[2]\n",
    "        ) * self.scale\n",
    "        \n",
    "        # Torso length\n",
    "        measurements['torso_length'] = np.linalg.norm(\n",
    "            joints[12] - joints[0]  # neck to pelvis\n",
    "        ) * self.scale\n",
    "        \n",
    "        # Arm length (averaged left/right)\n",
    "        left_arm = (\n",
    "            np.linalg.norm(joints[16] - joints[18]) +\n",
    "            np.linalg.norm(joints[18] - joints[20])\n",
    "        )\n",
    "        right_arm = (\n",
    "            np.linalg.norm(joints[17] - joints[19]) +\n",
    "            np.linalg.norm(joints[19] - joints[21])\n",
    "        )\n",
    "        measurements['arm_length'] = ((left_arm + right_arm) / 2) * self.scale\n",
    "        \n",
    "        # Leg length (averaged)\n",
    "        left_leg = (\n",
    "            np.linalg.norm(joints[1] - joints[4]) +\n",
    "            np.linalg.norm(joints[4] - joints[7])\n",
    "        )\n",
    "        right_leg = (\n",
    "            np.linalg.norm(joints[2] - joints[5]) +\n",
    "            np.linalg.norm(joints[5] - joints[8])\n",
    "        )\n",
    "        measurements['leg_length'] = ((left_leg + right_leg) / 2) * self.scale\n",
    "        \n",
    "        # Inseam\n",
    "        crotch = (joints[1] + joints[2]) / 2\n",
    "        crotch[1] -= 0.03\n",
    "        ankle = (joints[7] + joints[8]) / 2\n",
    "        measurements['inseam'] = np.linalg.norm(crotch - ankle) * self.scale\n",
    "        \n",
    "        # ===== CIRCUMFERENCE MEASUREMENTS (PCA) =====\n",
    "        \n",
    "        pca_measurer = PCACircumferenceMeasurer(scale_factor=self.scale)\n",
    "        circumferences = pca_measurer.measure_all(vertices, joints)\n",
    "        \n",
    "        measurements['chest_circumference'] = circumferences['chest']\n",
    "        measurements['waist_circumference'] = circumferences['waist']\n",
    "        measurements['hip_circumference'] = circumferences['hip']\n",
    "        measurements['thigh_circumference'] = circumferences['thigh']\n",
    "        measurements['bicep_circumference'] = circumferences['bicep']\n",
    "        \n",
    "        return measurements\n",
    "\n",
    "print(\"‚úÖ Measurement Extractor V2.1 ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Complete Pipeline V2.1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Upload Video\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload your turntable video:\")\n",
    "uploaded = files.upload()\n",
    "VIDEO_PATH = list(uploaded.keys())[0]\n",
    "print(f\"‚úÖ Uploaded: {VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Extract Frames (8 Strategic Views)\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "N_FRAMES = 8\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "indices = np.linspace(0, total_frames - 1, N_FRAMES + 1)[:-1].astype(int)\n",
    "\n",
    "frames = []\n",
    "for i, idx in enumerate(indices):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frames.append({\n",
    "            'index': i,\n",
    "            'data': cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        })\n",
    "cap.release()\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(frames)} frames ({width}x{height})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Detection + Pose Estimation\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "# Detection\n",
    "detector = YOLO('yolov8x.pt')\n",
    "\n",
    "# Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=2, enable_segmentation=True)\n",
    "\n",
    "pose_results = []\n",
    "print(\"üîç Processing frames...\")\n",
    "\n",
    "for frame in frames:\n",
    "    # Detect bbox\n",
    "    det = detector(frame['data'], verbose=False, classes=[0])[0]\n",
    "    if len(det.boxes) == 0:\n",
    "        pose_results.append(None)\n",
    "        continue\n",
    "    \n",
    "    bbox = det.boxes.xyxy[0].cpu().numpy()\n",
    "    \n",
    "    # Crop and run pose\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    crop = frame['data'][y1:y2, x1:x2]\n",
    "    result = pose.process(crop)\n",
    "    \n",
    "    if result.pose_landmarks:\n",
    "        ch, cw = crop.shape[:2]\n",
    "        kps = []\n",
    "        for lm in result.pose_landmarks.landmark:\n",
    "            kps.append([lm.x * cw + x1, lm.y * ch + y1, lm.visibility])\n",
    "        \n",
    "        # Convert to COCO 17\n",
    "        mp_kps = np.array(kps)\n",
    "        mapping = {0:0, 2:1, 5:2, 7:3, 8:4, 11:5, 12:6, 13:7, 14:8, 15:9, 16:10, 23:11, 24:12, 25:13, 26:14, 27:15, 28:16}\n",
    "        coco_kps = np.zeros((17, 3))\n",
    "        for mp_i, coco_i in mapping.items():\n",
    "            coco_kps[coco_i] = mp_kps[mp_i]\n",
    "        \n",
    "        pose_results.append({\n",
    "            'keypoints': coco_kps,\n",
    "            'bbox': bbox,\n",
    "            'segmentation': result.segmentation_mask if result.segmentation_mask is not None else None\n",
    "        })\n",
    "        print(f\"   Frame {frame['index']}: ‚úì\")\n",
    "    else:\n",
    "        pose_results.append(None)\n",
    "\n",
    "print(f\"\\n‚úÖ Pose estimation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Upload SMPL-X Model\n",
    "import os\n",
    "import smplx\n",
    "\n",
    "SMPLX_PATH = 'workspace/models/smplx'\n",
    "\n",
    "if not os.path.exists(os.path.join(SMPLX_PATH, 'SMPLX_NEUTRAL.npz')):\n",
    "    print(\"üì§ Upload SMPLX_NEUTRAL.npz:\")\n",
    "    uploaded = files.upload()\n",
    "    for fname in uploaded:\n",
    "        os.rename(fname, os.path.join(SMPLX_PATH, fname))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "body_model = smplx.create(\n",
    "    SMPLX_PATH, model_type='smplx', gender='neutral',\n",
    "    num_betas=10, ext='npz'\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ SMPL-X loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Iterative PnP Camera Estimation\n",
    "#@markdown This is the key improvement!\n",
    "\n",
    "pnp_estimator = IterativePnPEstimator(width, height)\n",
    "cameras, refined_betas = pnp_estimator.iterative_estimate(\n",
    "    pose_results, body_model, device, n_iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Final Shape Optimization\n",
    "\n",
    "# Full optimization with refined cameras\n",
    "betas = refined_betas.clone().detach().requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([betas], lr=0.02)\n",
    "\n",
    "print(\"üîß Final shape optimization...\")\n",
    "\n",
    "for iteration in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = body_model(\n",
    "        betas=betas,\n",
    "        body_pose=torch.zeros(1, 63, device=device),\n",
    "        global_orient=torch.zeros(1, 3, device=device)\n",
    "    )\n",
    "    joints = output.joints[0]\n",
    "    \n",
    "    loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for kp_data, cam in zip(pose_results, cameras):\n",
    "        if kp_data is None or cam is None:\n",
    "            continue\n",
    "        \n",
    "        R = torch.tensor(cam['R'], dtype=torch.float32, device=device)\n",
    "        t = torch.tensor(cam['t'], dtype=torch.float32, device=device)\n",
    "        K = torch.tensor(cam['K'], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # COCO joints from SMPL-X\n",
    "        smplx_to_coco = [15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 21, 1, 2, 4, 5, 7, 8]\n",
    "        coco_3d = torch.stack([joints[i] for i in smplx_to_coco])\n",
    "        \n",
    "        # Project\n",
    "        cam_pts = torch.matmul(coco_3d, R.T) + t\n",
    "        proj = torch.matmul(cam_pts, K.T)\n",
    "        proj_2d = proj[:, :2] / (proj[:, 2:3] + 1e-8)\n",
    "        \n",
    "        gt_2d = torch.tensor(kp_data['keypoints'][:, :2], dtype=torch.float32, device=device)\n",
    "        conf = torch.tensor(kp_data['keypoints'][:, 2], dtype=torch.float32, device=device)\n",
    "        \n",
    "        diff = proj_2d - gt_2d\n",
    "        loss += torch.sum(conf.unsqueeze(-1) * diff**2)\n",
    "        count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        # Add symmetry prior\n",
    "        left_arm = torch.norm(joints[16] - joints[18]) + torch.norm(joints[18] - joints[20])\n",
    "        right_arm = torch.norm(joints[17] - joints[19]) + torch.norm(joints[19] - joints[21])\n",
    "        symmetry_loss = (left_arm - right_arm) ** 2\n",
    "        \n",
    "        total_loss = loss / count + 0.01 * torch.mean(betas**2) + 0.1 * symmetry_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if iteration % 50 == 0:\n",
    "        print(f\"   Iter {iteration}: Loss = {total_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Shape optimization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Extract Measurements (V2.1)\n",
    "\n",
    "# Get canonical mesh\n",
    "with torch.no_grad():\n",
    "    output = body_model(\n",
    "        betas=betas,\n",
    "        body_pose=torch.zeros(1, 63, device=device),\n",
    "        global_orient=torch.zeros(1, 3, device=device),\n",
    "        return_verts=True\n",
    "    )\n",
    "\n",
    "vertices = output.vertices[0].cpu().numpy()\n",
    "joints = output.joints[0].cpu().numpy()\n",
    "\n",
    "# Extract measurements with PCA circumferences\n",
    "KNOWN_HEIGHT_CM = None  #@param {type:\"number\"}\n",
    "\n",
    "extractor = MeasurementExtractorV21(known_height_cm=KNOWN_HEIGHT_CM)\n",
    "measurements = extractor.extract(vertices, joints)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìè BODY MEASUREMENTS (V2.1 - PCA Circumferences)\")\n",
    "print(\"=\"*60)\n",
    "for name, value in measurements.items():\n",
    "    print(f\"   {name.replace('_', ' ').title():<25} {value:>8.1f} cm\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Save Results\n",
    "import json\n",
    "\n",
    "output = {\n",
    "    'measurements_cm': {k: float(v) for k, v in measurements.items()},\n",
    "    'pipeline_version': '2.1',\n",
    "    'improvements': [\n",
    "        'Iterative PnP camera estimation (3 rounds)',\n",
    "        'Reliable joint mapping (skip head joints)',\n",
    "        'PCA-based circumference measurement',\n",
    "        'Symmetry priors'\n",
    "    ],\n",
    "    'betas': betas.cpu().numpy().tolist()\n",
    "}\n",
    "\n",
    "with open('workspace/output/final/measurements_v21.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "# Save mesh\n",
    "with open('workspace/output/final/body_v21.obj', 'w') as f:\n",
    "    for v in vertices:\n",
    "        f.write(f\"v {v[0]} {v[1]} {v[2]}\\n\")\n",
    "    for face in body_model.faces:\n",
    "        f.write(f\"f {face[0]+1} {face[1]+1} {face[2]+1}\\n\")\n",
    "\n",
    "print(\"‚úÖ Results saved!\")\n",
    "print(\"   - measurements_v21.json\")\n",
    "print(\"   - body_v21.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download Results\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive('body_v21', 'zip', 'workspace/output/final')\n",
    "files.download('body_v21.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Pipeline V2.1 Complete!\n",
    "---\n",
    "\n",
    "## All Issues Fixed\n",
    "\n",
    "| Issue | V2 | V2.1 |\n",
    "|-------|----|----|  \n",
    "| PnP initialization | Single-shot with neutral pose | **Iterative refinement (3 rounds)** |\n",
    "| Head joint mapping | Noisy approximation | **Skip head, use reliable joints only** |\n",
    "| Convex hull | Overestimates armpits/waist | **GrabCut option + better fallback** |\n",
    "| Height-based circumference | Fails with tilt | **PCA plane slicing** |\n",
    "\n",
    "## Expected Accuracy\n",
    "| Measurement | Error |\n",
    "|-------------|-------|\n",
    "| Height | ¬±1.0-1.5 cm |\n",
    "| Chest/Waist/Hip | ¬±2-3 cm |\n",
    "| Inseam | ¬±1.5-2 cm |\n",
    "| Arm/Leg length | ¬±1.5 cm |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
